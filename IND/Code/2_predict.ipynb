{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca1baaf-628f-45af-adf1-f2d86e115156",
   "metadata": {},
   "source": [
    "# LB:(20240608_miyaki_5.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5816e52-dd1a-49dc-8473-44728f658654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi:8 | device:cuda\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoost,Pool\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score,confusion_matrix\n",
    "\n",
    "CORE_NUM = multiprocessing.cpu_count()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "warnings.simplefilter('ignore')\n",
    "os.makedirs(\"../submit/\", exist_ok=True)\n",
    "\n",
    "print(f'multi:{CORE_NUM} | device:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a5fcc8-8f47-4088-b252-a823c5117be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:(779, 4) | test:(515, 3)\n"
     ]
    }
   ],
   "source": [
    "df_master = pd.read_parquet('../test_data/cleaned_pid_to_info_all_v6.parquet')\n",
    "df_master = df_master.rename(columns={'id':'paper_id'})\n",
    "df_train_master = pd.read_parquet('../test_data/train_author.parquet')\n",
    "df_test_master = pd.read_parquet('../test_data/ind_test_author_filter_public.parquet')\n",
    "print(f'train:{df_train_master.shape} | test:{df_test_master.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ba7eac-db28-47a0-b485-15c3134e39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:(148309, 1158) | test:(116262, 1158)\n"
     ]
    }
   ],
   "source": [
    "def get_ids(df, proc):\n",
    "    merge_keys = ['author_id','paper_id','label']\n",
    "    if proc == 'train':\n",
    "        df_normal = df.explode('normal_data')[['id','normal_data']]\n",
    "        df_normal.columns = ['id','paper_id']\n",
    "        df_normal['label'] = 1\n",
    "        df_outliers = df.explode('outliers')[['id','outliers']]\n",
    "        df_outliers.columns = ['id','paper_id']\n",
    "        df_outliers['label'] = 0\n",
    "        df = pd.concat([df_normal,df_outliers])\n",
    "        df.columns = ['author_id','paper_id','label']\n",
    "    elif proc == 'test':\n",
    "        df = df.explode('papers')[['id','papers']]\n",
    "        df['label'] = 0\n",
    "        df.columns = ['author_id','paper_id','label']\n",
    "    return df[merge_keys]\n",
    "    \n",
    "def get_feature(df, proc):\n",
    "    merge_keys = ['author_id','paper_id','label']\n",
    "    \n",
    "    df_glm = pd.read_parquet(f'../test_feature/{proc}_glm.parquet')\n",
    "    df = df.merge(df_glm, on=merge_keys, how='left')\n",
    "    \n",
    "    df_sci = pd.read_parquet(f'../test_feature/{proc}_scibert_nli.parquet')\n",
    "    df = df.merge(df_sci, on=merge_keys, how='left')\n",
    "\n",
    "    df_e5 = pd.read_parquet(f'../test_feature/{proc}_multilingual_e5_large.parquet')\n",
    "    df = df.merge(df_e5, on=merge_keys, how='left')\n",
    "\n",
    "    df_minilm = pd.read_parquet(f'../test_feature/{proc}_minilm.parquet')\n",
    "    df = df.merge(df_minilm, on=merge_keys, how='left')\n",
    "\n",
    "    df_deb = pd.read_parquet(f'../test_feature/{proc}_deberta.parquet')\n",
    "    df = df.merge(df_deb, on=merge_keys, how='left')\n",
    "\n",
    "    df_w2v = pd.read_parquet(f'../test_feature/{proc}_w2v.parquet')\n",
    "    df = df.merge(df_w2v, on=merge_keys, how='left')\n",
    "\n",
    "    df_oag = pd.read_parquet(f'../test_feature/{proc}_oag_bert.parquet')\n",
    "    df = df.merge(df_oag, on=merge_keys, how='left')\n",
    "\n",
    "    df_tfidf = pd.read_parquet(f'../test_feature/{proc}_tfidf.parquet')\n",
    "    df = df.merge(df_tfidf, on=merge_keys, how='left')\n",
    "\n",
    "    df_graph = pd.read_parquet(f'../test_feature/{proc}_graph.parquet')\n",
    "    df = df.merge(df_graph, on=merge_keys, how='left')\n",
    "\n",
    "    df_jaccard = pd.read_parquet(f'../test_feature/{proc}_jaccard.parquet')\n",
    "    df = df.merge(df_jaccard, on=merge_keys, how='left')\n",
    "\n",
    "    df_basic = pd.read_parquet(f'../test_feature/{proc}_basic.parquet')\n",
    "    df = df.merge(df_basic, on=merge_keys, how='left')\n",
    "    \n",
    "    # label data\n",
    "    label_col = [col for col in df.columns if '_label' in col]\n",
    "    df[label_col] = df[label_col].replace(-1, 0)\n",
    "    prefix_list = list(set([p.split('_')[0] for p in label_col]))\n",
    "    for p in prefix_list:\n",
    "        cols = [col for col in label_col if p in col]\n",
    "        df[f'{p}_label_mean'] = df[cols].sum(axis=1)/len(cols)\n",
    "        df[f'{p}_label_mean'] = df[f'{p}_label_mean'].apply(lambda x : 1 / (1 + np.exp(-x)))\n",
    "    type_list = list(set([p.split('_')[1] for p in label_col]))\n",
    "    for t in type_list:\n",
    "        cols = [col for col in label_col if t in col]\n",
    "        df[f'{t}_label_mean'] = df[cols].sum(axis=1)/len(cols)\n",
    "        df[f'{t}_label_mean'] = df[f'{t}_label_mean'].apply(lambda x : 1 / (1 + np.exp(-x)))\n",
    "    # pred data\n",
    "    pred_col = [col for col in df.columns if '_pred' in col]\n",
    "    suffix_col =  list(set([p.split('_')[-1] for p in pred_col]))     \n",
    "    for p in prefix_list:\n",
    "        for s in suffix_col:\n",
    "            cols = [col for col in pred_col if p in col]\n",
    "            cols = [col for col in cols if s in col]\n",
    "            if len(cols)>1:\n",
    "                df[f'{p}_pred_mean_{s}'] = df[cols].sum(axis=1)/len(cols)\n",
    "                df[f'{p}_pred_mean_{s}'] = df[f'{p}_pred_mean_{s}'].apply(lambda x : 1 / (1 + np.exp(-x)))\n",
    "    feature_col = [col for col in df.columns.tolist() if col not in merge_keys]    \n",
    "    return df,feature_col\n",
    "df_train = get_ids(df_train_master, 'train')\n",
    "df_train, feature_col = get_feature(df_train, 'train')\n",
    "df_test = get_ids(df_test_master, 'test')\n",
    "df_test, _ = get_feature(df_test, 'test')\n",
    "print(f'train:{df_train[feature_col].shape} | test:{df_test[feature_col].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a563194-0d30-4fda-b965-f9e0c135a82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "削減前:1158 | 削減後:1144\n",
      "削除：['deberta_title_cos_min', 'e5_venue_cos_min', 'mini_pred_mean_gm', 'deberta_venue_cos_min', 'deberta_all_cos_min', 'e5_all_cos_min', 'e5_abstract_cos_min', 'deberta_abstract_cos_min', 'e5_title_cos_min', 'deberta_keywords_cos_min', 'deberta_pred_mean_gm', 'sci_pred_mean_gm', 'e5_pred_mean_gm', 'e5_keywords_cos_min']\n"
     ]
    }
   ],
   "source": [
    "# 無意味な特徴量の削除\n",
    "def drop_noise(df, f_col):\n",
    "    one_value_cols = [col for col in f_col if df[col].nunique() <= 1]\n",
    "    many_null_cols = [col for col in f_col if df[col].isnull().sum() / df_train.shape[0] > 0.9]\n",
    "    noise_cols = list(set(many_null_cols + one_value_cols))\n",
    "    return noise_cols\n",
    "train_noise_cols = drop_noise(df_train, feature_col)\n",
    "test_noise_cols = drop_noise(df_test, feature_col)\n",
    "noise_cols = list(set(train_noise_cols + test_noise_cols))\n",
    "before_num = len(feature_col)\n",
    "feature_col = [col for col in feature_col if col not in noise_cols]\n",
    "after_num = len(feature_col)\n",
    "print(f'削減前:{before_num} | 削減後:{after_num}')\n",
    "print(f'削除：{noise_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d2524-9057-4f29-98c2-0e599b6cc3d5",
   "metadata": {},
   "source": [
    "# Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f704d7-ab16-4378-b6ab-377fe75fd364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:(97102, 1144) 負例:11320 | valid:(10790, 1144) 負例:1258 | test:(40417, 1144) 負例:4707\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証・テストデータ作成\n",
    "groups = df_train['author_id']\n",
    "X = df_train.set_index(['author_id','paper_id'])\n",
    "y = X['label']\n",
    "X = X[feature_col]\n",
    "# テストデータ作成\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3,random_state=85)\n",
    "train_idxs,test_idxs = next(gss.split(X=X, y=y, groups=groups))\n",
    "X, X_test, y, y_test = X.iloc[train_idxs],X.iloc[test_idxs], y.iloc[train_idxs],y.iloc[test_idxs]\n",
    "# 学習・検証データ作成\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.1,\n",
    "    random_state=1,\n",
    "    shuffle=True,\n",
    "    stratify=y\n",
    ")\n",
    "print(f'train:{X_train.shape} 負例:{len(y_train[y_train==0])} | valid:{X_valid.shape} 負例:{len(y_valid[y_valid==0])} | test:{X_test.shape} 負例:{len(y_test[y_test==0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd218d-b869-4460-8f78-3fa7050d2067",
   "metadata": {},
   "source": [
    "# 評価関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff04644a-86a5-445b-b443-45f4161da0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価関数\n",
    "def get_evaluate(y_test, predict):\n",
    "    auc = roc_auc_score(y_test, predict)\n",
    "    predict = np.where(predict < 0.5, 0, 1)\n",
    "    precision = precision_score(y_test, predict)\n",
    "    recall = recall_score(y_test, predict)      \n",
    "    return auc, precision, recall\n",
    "\n",
    "def compute_score(target: np.ndarray,pred: np.ndarray,groups: np.ndarray) -> float:\n",
    "    result_df = pd.DataFrame({\"pred\": pred, \"target\": target, \"group\": groups})\n",
    "    total_errors = len(result_df[result_df[\"target\"]==0])\n",
    "    score = 0.0\n",
    "    for _, gdf in result_df.groupby(\"group\"):\n",
    "        weight = len(gdf[gdf[\"target\"]==0]) / total_errors\n",
    "        auc = roc_auc_score(gdf[\"target\"], gdf[\"pred\"])\n",
    "        score += auc * weight\n",
    "    return score\n",
    "    \n",
    "def eval(y_pred, X_test, y_test):\n",
    "    local_score = compute_score(y_test.values, np.array(y_pred), y_test.index.get_level_values('author_id').tolist())\n",
    "    auc, precision, recall = get_evaluate(y_test, y_pred)\n",
    "    print(f'LOCAL SCORE:{local_score}')\n",
    "    print(f'AUC:{auc} | PRECISION:{precision} | RECALL:{recall}')\n",
    "    \n",
    "    y_pred = np.where(y_pred < 0.5, 0, 1)\n",
    "    test = X_test.copy()\n",
    "    test['pred'] = y_pred\n",
    "    test['label'] = y_test.values\n",
    "    test = test.reset_index()\n",
    "    tmp = []\n",
    "    for author_id, group in test.groupby('author_id'):\n",
    "        group.loc[group['label'] == group['pred'], 'flag'] = 1\n",
    "        group.loc[group['label'] != group['pred'], 'flag'] = 0\n",
    "        error = (len(group[group['flag']==0])/len(group))*100\n",
    "        group['error'] = error\n",
    "        group = group[['author_id','paper_id','pred','label','flag','error']]\n",
    "        tmp.append(group)\n",
    "    test = pd.concat(tmp)\n",
    "    test = test.merge(df_master,on='paper_id',how='left')\n",
    "    test = test.sort_values('error',ascending=False)\n",
    "    cm = confusion_matrix(y_test.values, y_pred)\n",
    "    print(cm)\n",
    "    return local_score, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d5c93-44a8-4bd3-b64a-63417733433f",
   "metadata": {},
   "source": [
    "# モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e690793b-410a-41e4-8131-c76ae29db72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 02:13:49.214681: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.224103: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.227081: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.230457: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.233348: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.236149: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.439321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.440483: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.441543: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-08 02:13:49.443318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m16,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">149,121</span> (582.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m149,121\u001b[0m (582.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">148,481</span> (580.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m148,481\u001b[0m (580.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> (2.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m640\u001b[0m (2.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717812831.648759    7533 service.cc:145] XLA service 0x7f8668004b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1717812831.648792    7533 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-06-08 02:13:51.735622: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-08 02:13:52.072932: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8905\n",
      "I0000 00:00:1717812835.081423    7533 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "Epoch 28/1000\n",
      "Epoch 29/1000\n",
      "Epoch 30/1000\n",
      "Epoch 31/1000\n",
      "Epoch 32/1000\n",
      "Epoch 33/1000\n",
      "Epoch 34/1000\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "LOCAL SCORE:0.7423929586535796\n",
      "AUC:0.8814220400308246 | PRECISION:0.9169892812936188 | RECALL:0.9750490058807056\n",
      "[[ 1555  3152]\n",
      " [  891 34819]]\n"
     ]
    }
   ],
   "source": [
    "def nn_train(n, X_train, X_valid, y_train, y_valid, X_test, y_test=None, log=False):    \n",
    "    # 欠損値補完\n",
    "    df_mean = X_train.mean(numeric_only=True)\n",
    "    X_train_nn = X_train.fillna(df_mean)\n",
    "    X_valid_nn = X_valid.fillna(df_mean)\n",
    "    X_test_nn = X_test.fillna(df_mean)\n",
    "    # 次元圧縮\n",
    "    dim=512\n",
    "    pca = PCA(n_components=dim)\n",
    "    pca.fit(X_train_nn)\n",
    "    X_train_nn = pca.transform(X_train_nn)\n",
    "    X_valid_nn = pca.transform(X_valid_nn)\n",
    "    X_test_nn = pca.transform(X_test_nn)\n",
    "    # 標準化\n",
    "    ss = StandardScaler()\n",
    "    X_train_nn = ss.fit_transform(X_train_nn)\n",
    "    X_valid_nn = ss.transform(X_valid_nn)\n",
    "    X_test_nn = ss.transform(X_test_nn) \n",
    "    # モデルの構築\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=X_train_nn.shape[1], activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.6),\n",
    "        Dense(64, input_dim=X_train_nn.shape[1], activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    if log:\n",
    "        model.summary()\n",
    "    early_stopping = EarlyStopping(monitor='val_auc', patience=10, mode='max', restore_best_weights=True)\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss=BinaryCrossentropy(),\n",
    "                  metrics=[AUC(name='auc')])\n",
    "    # モデルのトレーニング\n",
    "    history = model.fit(X_train_nn, y_train,\n",
    "                        epochs=1000,\n",
    "                        batch_size=256,\n",
    "                        validation_data=(X_valid_nn, y_valid),\n",
    "                        verbose=10,\n",
    "                        callbacks=[early_stopping]\n",
    "                       )\n",
    "    y_pred = model.predict(X_test_nn).flatten()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    if y_test is not None:\n",
    "        _, test = eval(y_pred, X_test, y_test)\n",
    "        return y_pred\n",
    "    else:\n",
    "        return y_pred\n",
    "nn_pred = nn_train(1, X_train, X_valid, y_train, y_valid, X_test, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "807e0968-f40e-454b-af10-f7b93aef38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 77206407075.53612| val_0_unsup_loss_numpy: 26116888.0|  0:00:20s\n",
      "epoch 5  | loss: 3706268906.61633| val_0_unsup_loss_numpy: 18463060.0|  0:01:56s\n",
      "epoch 10 | loss: 3176558105.04447| val_0_unsup_loss_numpy: 2006614.625|  0:03:32s\n",
      "epoch 15 | loss: 1511897929.81026| val_0_unsup_loss_numpy: 77266784.0|  0:05:07s\n",
      "epoch 20 | loss: 1598853398.83735| val_0_unsup_loss_numpy: 2119974528.0|  0:06:43s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_unsup_loss_numpy = 613729.875\n",
      "epoch 0  | loss: 0.33211 | valid_auc: 0.82717 |  0:00:16s\n",
      "epoch 10 | loss: 0.14663 | valid_auc: 0.9492  |  0:02:59s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_valid_auc = 0.95446\n",
      "LOCAL SCORE:0.7579040097882414\n",
      "AUC:0.8841916062857222 | PRECISION:0.9286144413423113 | RECALL:0.9500420050406049\n",
      "[[ 2099  2608]\n",
      " [ 1784 33926]]\n"
     ]
    }
   ],
   "source": [
    "def tab_train(n, X_train, X_valid, y_train, y_valid, X_test, y_test=None, log=False):\n",
    "    # 事前学習\n",
    "    # f_feature = [col for col in feature_col if col not in null_col]\n",
    "    df_mean = X_train.mean(numeric_only=True)\n",
    "    X_train_tab = X_train.fillna(df_mean)\n",
    "    X_valid_tab = X_valid.fillna(df_mean)\n",
    "    X_test_tab = X_test.fillna(df_mean)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    unsupervised_model = TabNetPretrainer(\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        device_name = device,\n",
    "        mask_type='entmax',\n",
    "        verbose = 5,\n",
    "    )\n",
    "    unsupervised_model.fit(\n",
    "        X_train_tab.values,\n",
    "        eval_set=[X_valid_tab.values],\n",
    "        batch_size = 128,\n",
    "        pretraining_ratio=0.8,\n",
    "        # num_workers=CORE_NUM-1,\n",
    "    )\n",
    "    # 本学習\n",
    "    model = TabNetClassifier(\n",
    "        # n_d=16,\n",
    "        # n_a=16,\n",
    "        # n_steps=4, \n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        device_name = device,\n",
    "        verbose = 10,\n",
    "        seed = n\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_tab.values, y_train.values,\n",
    "        eval_set = [(X_valid_tab.values, y_valid.values)],\n",
    "        eval_metric= ['auc'],\n",
    "        eval_name = ['valid'],\n",
    "        batch_size = 128,\n",
    "        max_epochs = 100,\n",
    "        patience = 5,\n",
    "    num_workers=CORE_NUM-1,\n",
    "        from_unsupervised=unsupervised_model\n",
    "    )\n",
    "    y_pred = model.predict_proba(X_test_tab.values)[:, 1]\n",
    "    if y_test is not None:\n",
    "        _, test = eval(y_pred, X_test, y_test)\n",
    "        return y_pred\n",
    "    else:\n",
    "        return y_pred\n",
    "\n",
    "tab_pred = tab_train(1, X_train, X_valid, y_train, y_valid, X_test, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec4738f-5345-4fe6-b093-85f0eaf34eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:(97102, 1144)\n",
      "LOCAL SCORE:0.7799972824759734\n",
      "AUC:0.8911495667986638 | PRECISION:0.9105442880794702 | RECALL:0.9856622794735368\n",
      "[[ 1249  3458]\n",
      " [  512 35198]]\n"
     ]
    }
   ],
   "source": [
    "def rf_train(n, X_train, X_valid, y_train, y_valid, X_test, y_test=None, log=False):\n",
    "    print(f'TRAIN SHAPE:{X_train.shape}')\n",
    "    random_forest_model = RandomForestClassifier(n_estimators=150,random_state=42, max_depth=10)\n",
    "    random_forest_model.fit(X_train[feature_col], y_train)\n",
    "    y_pred = random_forest_model.predict_proba(X_test[feature_col])[:, 1]\n",
    "    if y_test is not None:\n",
    "        _, test = eval(y_pred, X_test, y_test)\n",
    "        return y_pred\n",
    "    else:\n",
    "        return y_pred\n",
    "rf_pred = rf_train(1, X_train, X_valid, y_train, y_valid, X_test, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04168c24-0d06-4c0e-9b41-66e25cb724d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:(97102, 1144)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\tvalid_0's auc: 0.976749\n",
      "Early stopping, best iteration is:\n",
      "[1448]\tvalid_0's auc: 0.978881\n",
      "Evaluated only: auc\n",
      "LOCAL SCORE:0.8026274278914615\n",
      "AUC:0.9161377618979032 | PRECISION:0.9303740062956837 | RECALL:0.9766451974236908\n",
      "[[ 2097  2610]\n",
      " [  834 34876]]\n"
     ]
    }
   ],
   "source": [
    "def lgbm_train(n, X_train, X_valid, y_train, y_valid, X_test, y_test=None, log=False):\n",
    "    print(f'TRAIN SHAPE:{X_train.shape}')\n",
    "    lgb_train = lgb.Dataset(\n",
    "        X_train, \n",
    "        y_train\n",
    "    )\n",
    "    lgb_eval = lgb.Dataset(\n",
    "        X_valid, \n",
    "        y_valid, \n",
    "        reference=lgb_train,\n",
    "    )\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': 7,\n",
    "        'num_leaves': 26,\n",
    "        'learning_rate': 0.02,\n",
    "        'verbosity': -1,\n",
    "        'random_state': n,\n",
    "        'n_jobs':CORE_NUM - 1\n",
    "    }\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_eval],\n",
    "        num_boost_round=10000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(\n",
    "                stopping_rounds=10,\n",
    "                first_metric_only=True, \n",
    "                verbose=True),\n",
    "            lgb.log_evaluation(1000)\n",
    "        ]\n",
    "    )\n",
    "    importance = pd.DataFrame(model.feature_importance(), index=X_train.columns.tolist(), columns=['importance'])\n",
    "    importance = importance.sort_values('importance',ascending=False)\n",
    "    if log:\n",
    "        print(f'NUM_OF_PARAMS:{len(model.params)}')\n",
    "        print(f'PARAMS:{model.params}')\n",
    "        display(importance.head())\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    if y_test is not None:\n",
    "        _, test = eval(y_pred, X_test, y_test)\n",
    "        return y_pred, importance, test, model.params, model\n",
    "    else:\n",
    "        return y_pred, importance\n",
    "\n",
    "lgb_pred, lgb_imp, lgb_error, params, model = lgbm_train(1, X_train, X_valid, y_train, y_valid, X_test, y_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c434289f-03e8-484f-8bd2-07642893bac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.036037\n",
      "0:\ttest: 0.8319194\tbest: 0.8319194 (0)\ttotal: 249ms\tremaining: 41m 30s\n",
      "1000:\ttest: 0.9745314\tbest: 0.9745314 (1000)\ttotal: 4m 15s\tremaining: 38m 19s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.9750076222\n",
      "bestIteration = 1060\n",
      "\n",
      "Shrink model to first 1061 iterations.\n",
      "LOCAL SCORE:0.7977686733292223\n",
      "AUC:0.9142010650795834 | PRECISION:0.925352560704061 | RECALL:0.9775413049565947\n",
      "[[ 1891  2816]\n",
      " [  802 34908]]\n"
     ]
    }
   ],
   "source": [
    "def catboost_train(n,X_train, X_valid, y_train, y_valid, X_test, y_test=None,log=False):\n",
    "    train_pool = Pool(X_train, label=y_train)\n",
    "    valid_pool = Pool(X_valid, label=y_valid)\n",
    "    params = {\n",
    "        # タスク設定と損失関数\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric':'AUC',\n",
    "        'depth':7,\n",
    "        'num_boost_round': 10000,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'random_state':n,\n",
    "        'thread_count':CORE_NUM-1,\n",
    "        'use_best_model':True,\n",
    "    }\n",
    "    # モデルを学習する\n",
    "    model = CatBoost(params)\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=valid_pool,\n",
    "        verbose=1000,\n",
    "    )\n",
    "    importance = pd.DataFrame(\n",
    "         model.get_feature_importance(type='PredictionValuesChange'),\n",
    "         index=X_train.columns.tolist(),\n",
    "         columns=['importance'])\n",
    "    importance = importance.sort_values('importance',ascending=False)\n",
    "    if log:\n",
    "        print(f'NUM_OF_PARAMS:{len(model.get_params())}')\n",
    "        print(f'PARAMS:{model.get_params()}')\n",
    "        display(importance.head())\n",
    "    y_pred = model.predict(X_test.values.tolist(),prediction_type='Probability')[:,1]\n",
    "    if y_test is not None:\n",
    "        _, test = eval(y_pred, X_test, y_test)\n",
    "        return y_pred, importance, test\n",
    "    else:\n",
    "        return y_pred, importance\n",
    "cat_pred, cat_imp, cat_error = catboost_train(1, X_train, X_valid, y_train, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823916c1-ba68-4548-9d94-a5fb1bd565bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.87724\n",
      "[1000]\teval-auc:0.97205\n",
      "[2000]\teval-auc:0.97746\n",
      "[2071]\teval-auc:0.97769\n",
      "LOCAL SCORE:0.7848442542246195\n",
      "AUC:0.9118602679315357 | PRECISION:0.9272944614158587 | RECALL:0.9775413049565947\n",
      "[[ 1970  2737]\n",
      " [  802 34908]]\n"
     ]
    }
   ],
   "source": [
    "def xgbt_train(n,X_train, X_valid, y_train, y_valid, X_test, y_test=None,log=False):\n",
    "    xgb_train = xgb.DMatrix(X_train,label=y_train)\n",
    "    xgb_eval = xgb.DMatrix(X_valid,label=y_valid)\n",
    "    xgb_test = xgb.DMatrix(X_test)\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'max_depth': 7,\n",
    "        'max_leaves':26,\n",
    "        'learning_rate': 0.02,\n",
    "        'seed': n,\n",
    "        # 'tree_method': 'gpu_hist',\n",
    "    }\n",
    "    model = xgb.train(\n",
    "        xgb_params,\n",
    "        xgb_train,\n",
    "        num_boost_round=10000,\n",
    "        evals=[(xgb_eval, 'eval')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=1000,\n",
    "    )\n",
    "    importance = pd.DataFrame(\n",
    "        list(model.get_score(importance_type='gain').items()), \n",
    "        columns=['feature','importance']\n",
    "    )\n",
    "    importance = importance.sort_values('importance',ascending=False)\n",
    "    if log:\n",
    "        print(f'NUM_OF_PARAMS:{len(model.attributes())}')\n",
    "        print(f'PARAMS:{model.attributes()}')\n",
    "        display(importance.head())\n",
    "    y_pred = model.predict(xgb_test,iteration_range=(0, model.best_iteration))\n",
    "    if y_test is not None:\n",
    "        _, test = eval(y_pred, X_test, y_test)\n",
    "        return y_pred, importance, test\n",
    "    else:\n",
    "        return y_pred, importance\n",
    "xgb_pred, xgb_imp, xgb_error = xgbt_train(1, X_train, X_valid, y_train, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53b9cfde-10a1-412c-84bd-d098c537635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL SCORE:0.805540838181124\n",
      "AUC:0.9176674432289427 | PRECISION:0.9278616218514188 | RECALL:0.977905348641837\n",
      "[[ 1992  2715]\n",
      " [  789 34921]]\n",
      "LOCAL SCORE:0.797009099509209\n",
      "AUC:0.9161992687476013 | PRECISION:0.9286113253717052 | RECALL:0.977681321758611\n",
      "[[ 2023  2684]\n",
      " [  797 34913]]\n",
      "LOCAL SCORE:0.7949005400142309\n",
      "AUC:0.9153444791110221 | PRECISION:0.9258640797285835 | RECALL:0.9781853822458695\n",
      "[[ 1910  2797]\n",
      " [  779 34931]]\n",
      "LOCAL SCORE:0.801151506780494\n",
      "AUC:0.9173171483786041 | PRECISION:0.9269282814614344 | RECALL:0.9782973956874825\n",
      "[[ 1953  2754]\n",
      " [  775 34935]]\n",
      "LOCAL SCORE:0.8000017683267081\n",
      "AUC:0.9132209534147709 | PRECISION:0.9246678464830027 | RECALL:0.9803136376365164\n",
      "[[ 1855  2852]\n",
      " [  703 35007]]\n",
      "LOCAL SCORE:0.796559869934678\n",
      "AUC:0.9151171682135743 | PRECISION:0.9253904907894389 | RECALL:0.9805096611593391\n",
      "[[ 1884  2823]\n",
      " [  696 35014]]\n",
      "LOCAL SCORE:0.7922506659425376\n",
      "AUC:0.9146048025019429 | PRECISION:0.9235306509374341 | RECALL:0.9821338560627275\n",
      "[[ 1803  2904]\n",
      " [  638 35072]]\n",
      "LOCAL SCORE:0.8000017683267081\n",
      "AUC:0.9132209534147709 | PRECISION:0.9246678464830027 | RECALL:0.9803136376365164\n",
      "[[ 1855  2852]\n",
      " [  703 35007]]\n",
      "LOCAL SCORE:0.7899232415156653\n",
      "AUC:0.9121693668462225 | PRECISION:0.9246851983844143 | RECALL:0.9809017082049846\n",
      "[[ 1854  2853]\n",
      " [  682 35028]]\n",
      "LOCAL SCORE:0.7840333923678146\n",
      "AUC:0.9117131387400226 | PRECISION:0.9224394222421005 | RECALL:0.9818258190982918\n",
      "[[ 1759  2948]\n",
      " [  649 35061]]\n"
     ]
    }
   ],
   "source": [
    "# ミックス\n",
    "# lgb + cat\n",
    "mean_pred = np.mean([lgb_pred, cat_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# lgb + xgb\n",
    "mean_pred = np.mean([lgb_pred, xgb_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# xgb + cat\n",
    "mean_pred = np.mean([cat_pred, xgb_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# lgb + xgb + cat\n",
    "mean_pred = np.mean([lgb_pred,cat_pred,xgb_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# lgb + xgb + cat + rf\n",
    "mean_pred = np.mean([lgb_pred, cat_pred, xgb_pred, rf_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# lgb + xgb + cat + rf + tab\n",
    "mean_pred = np.mean([lgb_pred, cat_pred, xgb_pred, rf_pred, tab_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# lgb + xgb + cat + rf + tab + nn\n",
    "mean_pred = np.mean([lgb_pred, cat_pred, xgb_pred, rf_pred, tab_pred, nn_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# xgb + cat + rf\n",
    "mean_pred = np.mean([lgb_pred, cat_pred, xgb_pred, rf_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# xgb + cat + rf + tab\n",
    "mean_pred = np.mean([cat_pred, xgb_pred, rf_pred, tab_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)\n",
    "# xgb + cat + rf + tab + nn\n",
    "mean_pred = np.mean([cat_pred, xgb_pred, rf_pred, tab_pred, nn_pred], axis=0)\n",
    "_, _ = eval(mean_pred, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924f82e-f690-483b-9b80-2552d31a23d1",
   "metadata": {},
   "source": [
    "# SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73cdbb61-9f52-4f08-b38a-b9de265a2a37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE:(118647, 1144)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\tvalid_0's auc: 0.971527\n",
      "[2000]\tvalid_0's auc: 0.976126\n",
      "Early stopping, best iteration is:\n",
      "[2089]\tvalid_0's auc: 0.976359\n",
      "Evaluated only: auc\n",
      "Learning rate set to 0.037866\n",
      "0:\ttest: 0.8244848\tbest: 0.8244848 (0)\ttotal: 268ms\tremaining: 44m 37s\n",
      "1000:\ttest: 0.9703834\tbest: 0.9703834 (1000)\ttotal: 4m 34s\tremaining: 41m 7s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.9734196402\n",
      "bestIteration = 1437\n",
      "\n",
      "Shrink model to first 1438 iterations.\n",
      "[0]\teval-auc:0.84732\n",
      "[1000]\teval-auc:0.96642\n",
      "[2000]\teval-auc:0.97345\n",
      "[2580]\teval-auc:0.97520\n",
      "Epoch 1/1000\n",
      "Epoch 2/1000\n",
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "Epoch 28/1000\n",
      "Epoch 29/1000\n",
      "Epoch 30/1000\n",
      "Epoch 31/1000\n",
      "Epoch 32/1000\n",
      "\u001b[1m3634/3634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 910us/step\n",
      "TRAIN SHAPE:(118647, 1144)\n",
      "epoch 0  | loss: 102377267303.84357| val_0_unsup_loss_numpy: 7394663.5|  0:00:25s\n",
      "epoch 5  | loss: 2460579328.65629| val_0_unsup_loss_numpy: 848749.1875|  0:02:32s\n",
      "epoch 10 | loss: 2530010121.79012| val_0_unsup_loss_numpy: 2524740.25|  0:04:39s\n",
      "epoch 15 | loss: 1116523247.94217| val_0_unsup_loss_numpy: 7770787.0|  0:06:47s\n",
      "epoch 20 | loss: 1290396767.70893| val_0_unsup_loss_numpy: 3432507.25|  0:08:54s\n",
      "epoch 25 | loss: 669473963.10314| val_0_unsup_loss_numpy: 1478635.25|  0:11:01s\n",
      "epoch 30 | loss: 951032365.18957| val_0_unsup_loss_numpy: 61776.87890625|  0:13:09s\n",
      "epoch 35 | loss: 636379776.17156| val_0_unsup_loss_numpy: 455765.65625|  0:15:16s\n",
      "epoch 40 | loss: 675385928.0681| val_0_unsup_loss_numpy: 126917.4609375|  0:17:22s\n",
      "epoch 45 | loss: 797725153.85165| val_0_unsup_loss_numpy: 60219160.0|  0:19:28s\n",
      "\n",
      "Early stopping occurred at epoch 48 with best_epoch = 38 and best_val_0_unsup_loss_numpy = 30068.70703125\n",
      "epoch 0  | loss: 0.35388 | valid_auc: 0.78538 |  0:00:21s\n",
      "epoch 10 | loss: 0.22632 | valid_auc: 0.89649 |  0:04:01s\n",
      "epoch 20 | loss: 0.19369 | valid_auc: 0.92325 |  0:07:40s\n",
      "epoch 30 | loss: 0.15008 | valid_auc: 0.93884 |  0:11:20s\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 33 and best_valid_auc = 0.94732\n",
      "TRAIN SHAPE:(118647, 1144)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\tvalid_0's auc: 0.973233\n",
      "[2000]\tvalid_0's auc: 0.977217\n",
      "Early stopping, best iteration is:\n",
      "[2133]\tvalid_0's auc: 0.977514\n",
      "Evaluated only: auc\n",
      "Learning rate set to 0.037866\n",
      "0:\ttest: 0.8349688\tbest: 0.8349688 (0)\ttotal: 280ms\tremaining: 46m 37s\n",
      "1000:\ttest: 0.9707086\tbest: 0.9707119 (999)\ttotal: 4m 31s\tremaining: 40m 44s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.9738774025\n",
      "bestIteration = 1437\n",
      "\n",
      "Shrink model to first 1438 iterations.\n",
      "[0]\teval-auc:0.86056\n",
      "[1000]\teval-auc:0.96858\n",
      "[1869]\teval-auc:0.97425\n",
      "Epoch 1/1000\n",
      "Epoch 2/1000\n",
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "Epoch 28/1000\n",
      "Epoch 29/1000\n",
      "Epoch 30/1000\n",
      "\u001b[1m3634/3634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 897us/step\n",
      "TRAIN SHAPE:(118647, 1144)\n",
      "epoch 0  | loss: 76203641664.18959| val_0_unsup_loss_numpy: 7591082.5|  0:00:25s\n",
      "epoch 5  | loss: 5230029750.01818| val_0_unsup_loss_numpy: 128959.8046875|  0:02:32s\n",
      "epoch 10 | loss: 3602469371.3108| val_0_unsup_loss_numpy: 72685.703125|  0:04:39s\n",
      "epoch 15 | loss: 3029769980.53754| val_0_unsup_loss_numpy: 26110.720703125|  0:06:46s\n",
      "epoch 20 | loss: 1449839010.45339| val_0_unsup_loss_numpy: 13162.5966796875|  0:08:52s\n",
      "epoch 25 | loss: 2064819132.25664| val_0_unsup_loss_numpy: 1301341.5|  0:10:58s\n",
      "epoch 30 | loss: 1015093077.282| val_0_unsup_loss_numpy: 158654.90625|  0:13:04s\n",
      "epoch 35 | loss: 1199446907.93256| val_0_unsup_loss_numpy: 65702.203125|  0:15:11s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_unsup_loss_numpy = 4287.40380859375\n",
      "epoch 0  | loss: 0.34345 | valid_auc: 0.82256 |  0:00:21s\n",
      "epoch 10 | loss: 0.18227 | valid_auc: 0.93869 |  0:04:02s\n",
      "epoch 20 | loss: 0.13958 | valid_auc: 0.94949 |  0:07:42s\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_valid_auc = 0.94949\n",
      "TRAIN SHAPE:(118647, 1144)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\tvalid_0's auc: 0.973426\n",
      "[2000]\tvalid_0's auc: 0.977527\n",
      "Early stopping, best iteration is:\n",
      "[2685]\tvalid_0's auc: 0.978867\n",
      "Evaluated only: auc\n",
      "Learning rate set to 0.037866\n",
      "0:\ttest: 0.8445021\tbest: 0.8445021 (0)\ttotal: 267ms\tremaining: 44m 33s\n",
      "1000:\ttest: 0.9718471\tbest: 0.9718471 (1000)\ttotal: 4m 31s\tremaining: 40m 45s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.9759590293\n",
      "bestIteration = 1701\n",
      "\n",
      "Shrink model to first 1702 iterations.\n",
      "[0]\teval-auc:0.86619\n",
      "[1000]\teval-auc:0.96838\n",
      "[2000]\teval-auc:0.97468\n",
      "[2325]\teval-auc:0.97568\n",
      "Epoch 1/1000\n",
      "Epoch 2/1000\n",
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "Epoch 28/1000\n",
      "Epoch 29/1000\n",
      "Epoch 30/1000\n",
      "Epoch 31/1000\n",
      "\u001b[1m3634/3634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 902us/step\n",
      "TRAIN SHAPE:(118647, 1144)\n",
      "epoch 0  | loss: 216877269697.22955| val_0_unsup_loss_numpy: 769359360.0|  0:00:25s\n",
      "epoch 5  | loss: 3822367437.71388| val_0_unsup_loss_numpy: 169271.796875|  0:02:34s\n",
      "epoch 10 | loss: 2062687159.42839| val_0_unsup_loss_numpy: 722571.75|  0:04:43s\n",
      "epoch 15 | loss: 3611690557.95128| val_0_unsup_loss_numpy: 29427326.0|  0:06:51s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_unsup_loss_numpy = 169271.796875\n",
      "epoch 0  | loss: 0.29002 | valid_auc: 0.88472 |  0:00:22s\n",
      "epoch 10 | loss: 0.14307 | valid_auc: 0.94811 |  0:04:02s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_valid_auc = 0.95065\n",
      "TRAIN SHAPE:(118647, 1144)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\tvalid_0's auc: 0.974058\n",
      "Early stopping, best iteration is:\n",
      "[1839]\tvalid_0's auc: 0.978091\n",
      "Evaluated only: auc\n",
      "Learning rate set to 0.037866\n",
      "0:\ttest: 0.8416846\tbest: 0.8416846 (0)\ttotal: 244ms\tremaining: 40m 36s\n",
      "1000:\ttest: 0.9718797\tbest: 0.9718804 (999)\ttotal: 4m 32s\tremaining: 40m 46s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.975178629\n",
      "bestIteration = 1463\n",
      "\n",
      "Shrink model to first 1464 iterations.\n",
      "[0]\teval-auc:0.86123\n",
      "[1000]\teval-auc:0.96869\n",
      "[2000]\teval-auc:0.97529\n",
      "[2516]\teval-auc:0.97672\n",
      "Epoch 1/1000\n",
      "Epoch 2/1000\n",
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "\u001b[1m3634/3634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 911us/step\n",
      "TRAIN SHAPE:(118647, 1144)\n",
      "epoch 0  | loss: 196072852801.16956| val_0_unsup_loss_numpy: 13635262.0|  0:00:25s\n",
      "epoch 5  | loss: 4276726855.03867| val_0_unsup_loss_numpy: 5717817.0|  0:02:35s\n",
      "epoch 10 | loss: 2921609816.11426| val_0_unsup_loss_numpy: 2826673.25|  0:04:44s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_unsup_loss_numpy = 317400.5625\n",
      "epoch 0  | loss: 0.282   | valid_auc: 0.87108 |  0:00:22s\n",
      "epoch 10 | loss: 0.14746 | valid_auc: 0.95235 |  0:04:03s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_valid_auc = 0.95235\n",
      "TRAIN SHAPE:(118648, 1144)\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[1000]\tvalid_0's auc: 0.972276\n",
      "[2000]\tvalid_0's auc: 0.976486\n",
      "Early stopping, best iteration is:\n",
      "[2250]\tvalid_0's auc: 0.977037\n",
      "Evaluated only: auc\n",
      "Learning rate set to 0.037866\n",
      "0:\ttest: 0.8018282\tbest: 0.8018282 (0)\ttotal: 264ms\tremaining: 44m 2s\n",
      "1000:\ttest: 0.9706152\tbest: 0.9706170 (999)\ttotal: 4m 36s\tremaining: 41m 25s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.9727516003\n",
      "bestIteration = 1316\n",
      "\n",
      "Shrink model to first 1317 iterations.\n",
      "[0]\teval-auc:0.86367\n",
      "[1000]\teval-auc:0.96670\n",
      "[2000]\teval-auc:0.97329\n",
      "[2154]\teval-auc:0.97372\n",
      "Epoch 1/1000\n",
      "Epoch 2/1000\n",
      "Epoch 3/1000\n",
      "Epoch 4/1000\n",
      "Epoch 5/1000\n",
      "Epoch 6/1000\n",
      "Epoch 7/1000\n",
      "Epoch 8/1000\n",
      "Epoch 9/1000\n",
      "Epoch 10/1000\n",
      "Epoch 11/1000\n",
      "Epoch 12/1000\n",
      "Epoch 13/1000\n",
      "Epoch 14/1000\n",
      "Epoch 15/1000\n",
      "Epoch 16/1000\n",
      "Epoch 17/1000\n",
      "Epoch 18/1000\n",
      "Epoch 19/1000\n",
      "Epoch 20/1000\n",
      "Epoch 21/1000\n",
      "Epoch 22/1000\n",
      "Epoch 23/1000\n",
      "Epoch 24/1000\n",
      "Epoch 25/1000\n",
      "Epoch 26/1000\n",
      "Epoch 27/1000\n",
      "Epoch 28/1000\n",
      "Epoch 29/1000\n",
      "Epoch 30/1000\n",
      "Epoch 31/1000\n",
      "Epoch 32/1000\n",
      "Epoch 33/1000\n",
      "Epoch 34/1000\n",
      "Epoch 35/1000\n",
      "\u001b[1m3634/3634\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 905us/step\n",
      "TRAIN SHAPE:(118648, 1144)\n",
      "epoch 0  | loss: 162710694259.93365| val_0_unsup_loss_numpy: 1678321.125|  0:00:25s\n",
      "epoch 5  | loss: 5580450811.21274| val_0_unsup_loss_numpy: 771760.5625|  0:02:35s\n",
      "epoch 10 | loss: 3047912490.61624| val_0_unsup_loss_numpy: 234125.0|  0:04:43s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_val_0_unsup_loss_numpy = 5041.5302734375\n",
      "epoch 0  | loss: 0.28662 | valid_auc: 0.88691 |  0:00:22s\n",
      "epoch 10 | loss: 0.1474  | valid_auc: 0.94718 |  0:04:05s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_valid_auc = 0.9486\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "X = df_train[feature_col]\n",
    "y = df_train['label']\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=92)\n",
    "df_result = pd.DataFrame()\n",
    "for train_index, valid_index in kf.split(X, y):\n",
    "    \n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_valid, y_valid = X.iloc[valid_index], y.iloc[valid_index]\n",
    "    submit = df_test.copy()\n",
    "    \n",
    "    lgb_pred, _ = lgbm_train(i,X_train, X_valid, y_train, y_valid, submit[feature_col])\n",
    "    submit[f'pred_l_{i}'] = lgb_pred\n",
    "    \n",
    "    cat_pred, _ = catboost_train(i,X_train, X_valid, y_train, y_valid, submit[feature_col])\n",
    "    submit[f'pred_c_{i}'] = cat_pred\n",
    "    \n",
    "    xgb_pred, _ = xgbt_train(i,X_train, X_valid, y_train, y_valid, submit[feature_col])\n",
    "    submit[f'pred_x_{i}'] = xgb_pred\n",
    "\n",
    "    nn_pred = nn_train(i, X_train, X_valid, y_train, y_valid, submit[feature_col])\n",
    "    submit[f'pred_n_{i}'] = nn_pred\n",
    "\n",
    "    rf_pred = rf_train(i,X_train, X_valid, y_train, y_valid, submit[feature_col])\n",
    "    submit[f'pred_r_{i}'] = rf_pred\n",
    "\n",
    "    tab_pred = tab_train(1, X_train, X_valid, y_train, y_valid, submit[feature_col])\n",
    "    submit[f'pred_t_{i}'] = tab_pred\n",
    "    \n",
    "    submit = submit[[\n",
    "        'author_id',\n",
    "        'paper_id', \n",
    "        f'pred_n_{i}', \n",
    "        f'pred_t_{i}',\n",
    "        f'pred_c_{i}',\n",
    "        f'pred_l_{i}',\n",
    "        f'pred_x_{i}', \n",
    "        f'pred_r_{i}'\n",
    "    ]]\n",
    "    \n",
    "    if i == 0:\n",
    "        df_result = submit\n",
    "    else:\n",
    "        df_result = df_result.merge(submit, on=['author_id','paper_id'], how='left')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "850e52ff-7a38-4784-887a-9a586f2a3953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>pred_n_0</th>\n",
       "      <th>pred_t_0</th>\n",
       "      <th>pred_c_0</th>\n",
       "      <th>pred_l_0</th>\n",
       "      <th>pred_x_0</th>\n",
       "      <th>pred_r_0</th>\n",
       "      <th>pred_n_1</th>\n",
       "      <th>pred_t_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_c_3</th>\n",
       "      <th>pred_l_3</th>\n",
       "      <th>pred_x_3</th>\n",
       "      <th>pred_r_3</th>\n",
       "      <th>pred_n_4</th>\n",
       "      <th>pred_t_4</th>\n",
       "      <th>pred_c_4</th>\n",
       "      <th>pred_l_4</th>\n",
       "      <th>pred_x_4</th>\n",
       "      <th>pred_r_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>0DchSY2n</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.997906</td>\n",
       "      <td>0.998925</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.989472</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.998577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998874</td>\n",
       "      <td>0.999777</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.988388</td>\n",
       "      <td>0.999925</td>\n",
       "      <td>0.998548</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999799</td>\n",
       "      <td>0.999774</td>\n",
       "      <td>0.988611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>0Gw6iDes</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.998098</td>\n",
       "      <td>0.998589</td>\n",
       "      <td>0.998630</td>\n",
       "      <td>0.998909</td>\n",
       "      <td>0.988811</td>\n",
       "      <td>0.999143</td>\n",
       "      <td>0.995871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998650</td>\n",
       "      <td>0.999652</td>\n",
       "      <td>0.999363</td>\n",
       "      <td>0.988001</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.998683</td>\n",
       "      <td>0.999290</td>\n",
       "      <td>0.999301</td>\n",
       "      <td>0.987937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>0PgoDSAP</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.997839</td>\n",
       "      <td>0.996995</td>\n",
       "      <td>0.995894</td>\n",
       "      <td>0.994530</td>\n",
       "      <td>0.971817</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>0.997278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994646</td>\n",
       "      <td>0.993036</td>\n",
       "      <td>0.996453</td>\n",
       "      <td>0.961597</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>0.996407</td>\n",
       "      <td>0.993643</td>\n",
       "      <td>0.996104</td>\n",
       "      <td>0.995694</td>\n",
       "      <td>0.967773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>0S7g2B2l</td>\n",
       "      <td>0.936261</td>\n",
       "      <td>0.997916</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.997748</td>\n",
       "      <td>0.997187</td>\n",
       "      <td>0.987839</td>\n",
       "      <td>0.996448</td>\n",
       "      <td>0.993690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996661</td>\n",
       "      <td>0.998762</td>\n",
       "      <td>0.998011</td>\n",
       "      <td>0.984952</td>\n",
       "      <td>0.998846</td>\n",
       "      <td>0.997943</td>\n",
       "      <td>0.990603</td>\n",
       "      <td>0.998547</td>\n",
       "      <td>0.997021</td>\n",
       "      <td>0.984736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>0YJjxtdf</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.994194</td>\n",
       "      <td>0.998507</td>\n",
       "      <td>0.996973</td>\n",
       "      <td>0.966986</td>\n",
       "      <td>0.998855</td>\n",
       "      <td>0.986124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995817</td>\n",
       "      <td>0.998226</td>\n",
       "      <td>0.997173</td>\n",
       "      <td>0.968581</td>\n",
       "      <td>0.998481</td>\n",
       "      <td>0.984952</td>\n",
       "      <td>0.994068</td>\n",
       "      <td>0.998569</td>\n",
       "      <td>0.998380</td>\n",
       "      <td>0.974821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_id  paper_id  pred_n_0  pred_t_0  pred_c_0  pred_l_0  pred_x_0  \\\n",
       "0  Fkb16wn7  0DchSY2n  0.999974  0.997906  0.998925  0.999752  0.999738   \n",
       "1  Fkb16wn7  0Gw6iDes  0.999889  0.998098  0.998589  0.998630  0.998909   \n",
       "2  Fkb16wn7  0PgoDSAP  0.999959  0.997839  0.996995  0.995894  0.994530   \n",
       "3  Fkb16wn7  0S7g2B2l  0.936261  0.997916  0.995652  0.997748  0.997187   \n",
       "4  Fkb16wn7  0YJjxtdf  0.998884  0.997115  0.994194  0.998507  0.996973   \n",
       "\n",
       "   pred_r_0  pred_n_1  pred_t_1  ...  pred_c_3  pred_l_3  pred_x_3  pred_r_3  \\\n",
       "0  0.989472  0.999393  0.998577  ...  0.998874  0.999777  0.999673  0.988388   \n",
       "1  0.988811  0.999143  0.995871  ...  0.998650  0.999652  0.999363  0.988001   \n",
       "2  0.971817  0.999892  0.997278  ...  0.994646  0.993036  0.996453  0.961597   \n",
       "3  0.987839  0.996448  0.993690  ...  0.996661  0.998762  0.998011  0.984952   \n",
       "4  0.966986  0.998855  0.986124  ...  0.995817  0.998226  0.997173  0.968581   \n",
       "\n",
       "   pred_n_4  pred_t_4  pred_c_4  pred_l_4  pred_x_4  pred_r_4  \n",
       "0  0.999925  0.998548  0.998769  0.999799  0.999774  0.988611  \n",
       "1  0.999882  0.996312  0.998683  0.999290  0.999301  0.987937  \n",
       "2  0.999815  0.996407  0.993643  0.996104  0.995694  0.967773  \n",
       "3  0.998846  0.997943  0.990603  0.998547  0.997021  0.984736  \n",
       "4  0.998481  0.984952  0.994068  0.998569  0.998380  0.974821  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc3b7d1e-384f-4dd5-ab1c-4d55d335e2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author_id',\n",
       " 'paper_id',\n",
       " 'pred_n_0',\n",
       " 'pred_t_0',\n",
       " 'pred_c_0',\n",
       " 'pred_l_0',\n",
       " 'pred_r_0',\n",
       " 'pred_n_1',\n",
       " 'pred_t_1',\n",
       " 'pred_c_1',\n",
       " 'pred_l_1',\n",
       " 'pred_r_1',\n",
       " 'pred_n_2',\n",
       " 'pred_t_2',\n",
       " 'pred_c_2',\n",
       " 'pred_l_2',\n",
       " 'pred_r_2',\n",
       " 'pred_n_3',\n",
       " 'pred_t_3',\n",
       " 'pred_c_3',\n",
       " 'pred_l_3',\n",
       " 'pred_r_3',\n",
       " 'pred_n_4',\n",
       " 'pred_t_4',\n",
       " 'pred_c_4',\n",
       " 'pred_l_4',\n",
       " 'pred_r_4']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_col = [col for col in df_result.columns if '_x_' not in col]\n",
    "use_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff95a39e-5ce0-4443-ac4e-c2534e850f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dic = {}\n",
    "for name, group in df_result[use_col].groupby('author_id'):\n",
    "    p_dic[f'{name}'] = dict(zip(group['paper_id'].tolist(), group.mean(axis=1, numeric_only=True).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "853b3662-e3c4-4f5a-846d-9f3d7c62366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = open(\"../submit/submit.json\", \"w\")\n",
    "json.dump(p_dic, tf)\n",
    "tf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
