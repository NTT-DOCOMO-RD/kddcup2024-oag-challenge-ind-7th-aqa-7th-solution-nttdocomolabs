{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0da8df7-25e8-4484-a569-bf8cf7d25412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import tqdm\n",
    "import jieba\n",
    "import neologdn\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import ftlangdetect\n",
    "from janome.tokenizer import Tokenizer\n",
    "from pypinyin import lazy_pinyin\n",
    "from pykakasi import kakasi\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "kks = kakasi()\n",
    "import regex\n",
    "ps =PorterStemmer()\n",
    "tk = Tokenizer()\n",
    "\n",
    "os.makedirs(\"../test_data/\", exist_ok=True)\n",
    "nltk.download('stopwords')\n",
    "stopwords = ['at', 'based', 'in', 'of', 'for', 'on', 'and', 'to', 'an', 'using', 'with', 'the', 'by', 'we', 'be', 'is', 'are', 'can'] + stopwords.words('english')+ stopwords.words('spanish')+ stopwords.words('german')\n",
    "stopwords_extend = ['university', 'univ', 'china', 'department', 'dept', 'laboratory', 'lab',\n",
    "                    'school', 'al', 'et', 'institute', 'inst', 'college', 'chinese', 'beijing',\n",
    "                    'journal', 'science', 'international', 'key', 'sciences', 'research',\n",
    "                    'academy', 'state', 'center','key','univers','scienc','depart','institut','laboratori']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c186f96-786f-4382-b8e4-4edef88e20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list = []\n",
    "def clean_text(txt):\n",
    "    if txt != None:\n",
    "        puncs = '[!�“”\"#$%&\\'()【】（）／《》\\・*+,-./–:;<=>?@[\\\\]^_`{|}~—～’、。]+'\n",
    "        txt = txt.strip()\n",
    "        txt = txt.lower()\n",
    "        txt = txt.replace('\\n', ' ')\n",
    "        txt = txt.replace(u\"\\xa0\", u\" \")\n",
    "        txt = txt.replace('\\\\',' ')\n",
    "        txt = txt.replace('‐',' ')\n",
    "        txt = re.sub(puncs, ' ', txt)\n",
    "        txt = re.sub(r'\\s{2,}', ' ', txt).strip()\n",
    "        return txt\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def split_text(txt):\n",
    "    if txt == 'null' or txt == '':\n",
    "        return []\n",
    "    else:\n",
    "        txt = txt.split(' ')\n",
    "        txt = [word for word in txt if word not in stopwords]\n",
    "        txt = [word for word in txt if word.isdigit() != True]\n",
    "        txt = [re.sub(r'\\d+', '0', word) for word in txt]\n",
    "        txt = [ps.stem(word) for word in txt]\n",
    "        txt = [word for word in txt if word != '' and len(word) > 1]\n",
    "        return txt\n",
    "\n",
    "def split_list(lst):\n",
    "    if lst == ['null']:\n",
    "        return []\n",
    "    else:\n",
    "        lst = [word for word in lst if word not in stopwords]\n",
    "        lst = [word for word in lst if word.isdigit() != True]\n",
    "        lst = [re.sub(r'\\d+', '0', word) for word in lst]\n",
    "        lst = [ps.stem(word) for word in lst]\n",
    "        lst = [word for word in lst if word != '' and len(word) > 1]\n",
    "        return lst\n",
    "    \n",
    "def clean_authors(authors):\n",
    "    cleaned_authors, names_list, orgs_list = [], [], []\n",
    "    if len(authors) > 0:\n",
    "        for author in authors:\n",
    "            name = author.get('name')\n",
    "            if name != '':\n",
    "                name = clean_text(name)\n",
    "                ptn = r'[\\p{Block=Hiragana}\\p{Script=Katakana}\\p{Han}\\p{Script_Extensions=Han}\\u2E80-\\u2FDF\\u3005-\\u3007\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF\\U00020000-\\U0002EBEF]+'\n",
    "                re_ptn = regex.compile(ptn)\n",
    "                # 漢字・ひらがな・カタカナが存在する場合 =>日本人or中国人？\n",
    "                if re_ptn.search(name):\n",
    "                    # 英語削除（漢字 + ピンイン表記の入力が多い・どちらもフルネーム）\n",
    "                    re_txt = re_ptn.finditer(name)\n",
    "                    temp = []\n",
    "                    for t in re_txt:\n",
    "                        temp.append(t.group())\n",
    "                    name = ' '.join(temp)\n",
    "                    check_list.append(name)\n",
    "                    # jp or zh\n",
    "                    lang = ftlangdetect.detect(name, low_memory=False)['lang']\n",
    "                    if lang == 'zh':\n",
    "                        name = name.removesuffix(' 著')\n",
    "                        # 複数人の入力がある場合の対処\n",
    "                        if len(name) > 4 and len(name.split(' ')) > 2:\n",
    "                            tmp = []\n",
    "                            for n in name.split(' '):\n",
    "                                n = ' '.join(lazy_pinyin(n))\n",
    "                                n = re.sub(r'\\s{2,}', ' ', n).strip()\n",
    "                                n = split_name(n)\n",
    "                                tmp.extend(n)\n",
    "                            name = tmp\n",
    "                        else:            \n",
    "                            name = ' '.join(lazy_pinyin(name))\n",
    "                            name = re.sub(r'\\s{2,}', ' ', name).strip()\n",
    "                            name = split_name(name)\n",
    "                    # jaは精度甘め\n",
    "                    elif lang == 'ja':\n",
    "                        if is_zh(name) or len(name) < 4:\n",
    "                            name = ' '.join(lazy_pinyin(name))\n",
    "                            name = re.sub(r'\\s{2,}', ' ', name).strip()\n",
    "                            name = split_name(name)\n",
    "                        else:\n",
    "                            # ローマ字表記変換\n",
    "                            jps = []\n",
    "                            result = kks.convert(name)\n",
    "                            for w in result:\n",
    "                                if w['hepburn'] != ' ':\n",
    "                                    jps.append(w['hepburn'])\n",
    "                            name_1 = ' '.join(jps)\n",
    "                            name_1 = re.sub(r'\\s{2,}', ' ', name_1).strip()\n",
    "                            name = split_name(name_1)\n",
    "                else:\n",
    "                    name = split_name(name)\n",
    "            else:\n",
    "                name = ['NULL']\n",
    "            names_list.extend(name)\n",
    "            \n",
    "            org = author.get('org')\n",
    "            if org != '':\n",
    "                org = clean_text(org)\n",
    "                org = org.split(' ')\n",
    "                org = [word for word in org if word not in stopwords]\n",
    "                org = [word for word in org if word not in stopwords_extend]\n",
    "                org = [word for word in org if word.isdigit() != True]\n",
    "                org = [re.sub(r'\\d+', '0', word) for word in org]\n",
    "                org = [ps.stem(word) for word in org]\n",
    "                org = [word for word in org if word != '' and len(word) > 1]\n",
    "                if len(org) == 0:\n",
    "                    org = ['NULL']\n",
    "            else:\n",
    "                org = ['NULL']\n",
    "            orgs_list.extend(org)\n",
    "            name = ','.join(list(set(name)))\n",
    "            org = ','.join(list(set(org)))\n",
    "            cleaned_authors.append([name, org])\n",
    "    return cleaned_authors, list(set(names_list)), list(set(orgs_list))\n",
    "\n",
    "def clean_year(year):\n",
    "    if year == 0 or year == '':\n",
    "        return None\n",
    "    else:\n",
    "        return int(year)\n",
    "\n",
    "def split_name(name):\n",
    "    name = name.split(' ')\n",
    "    name = [n for n in name if n.isdigit() != True]\n",
    "    if len(name) == 2:\n",
    "        name = [\n",
    "            name[0] + name[1],\n",
    "            # name[0][0] + '_' + name[1],\n",
    "            name[1] + name[0],\n",
    "            # name[1][0] + '_' + name[0]\n",
    "        ]\n",
    "        name.sort()\n",
    "        name = name[0]\n",
    "    elif len(name) > 2:\n",
    "        name = [\n",
    "            name[0] + name[1] + name[2],\n",
    "            # name[0][0] + '_' + name[1][0] + '_' + name[2],\n",
    "            name[2] + name[1] + name[0],\n",
    "            # name[2][0] + '_' + name[1][0] + '_' + name[0],\n",
    "            # name[0][0] + '_' + name[1][0] + name[2],\n",
    "            # name[2][0] + '_' + name[1][0] + name[0],\n",
    "            # name[0] + '_'  + name[2],\n",
    "            # name[0][0] + '_'  + name[2],\n",
    "            # name[2] + '_' + name[0],\n",
    "            # name[2][0] + '_' + name[0],\n",
    "        ]\n",
    "        name.sort()\n",
    "        name = name[0]\n",
    "    elif len(name) == 1 and name != ['']:\n",
    "        name = name[0]\n",
    "    else:\n",
    "        name = 'NULL'\n",
    "    return [name]\n",
    "    \n",
    "def clean_jp(lst):\n",
    "    lst = [neologdn.normalize(word) for word in lst]\n",
    "    lst = list(itertools.chain.from_iterable([list(tk.tokenize(txt, wakati=True)) for word in lst]))\n",
    "    lst = [word for word in lst if word != '']\n",
    "    return lst\n",
    "    \n",
    "def clean_zh(lst):\n",
    "    lst = list(itertools.chain.from_iterable([jieba.lcut(word) for word in lst]))\n",
    "    lst = [word for word in lst if word != '']\n",
    "    return lst\n",
    "\n",
    "def judge_lang(txt, lst):\n",
    "    ptn = r'[\\p{Block=Hiragana}\\p{Script=Katakana}\\p{Script_Extensions=Han}\\u2E80-\\u2FDF\\u3005-\\u3007\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF\\U00020000-\\U0002EBEF]+'\n",
    "    re_ptn = regex.compile(ptn)\n",
    "    if len(lst) == 0:\n",
    "        lang = 'Nothing'\n",
    "    else:\n",
    "        re_txt = re_ptn.search(txt)\n",
    "        if re_txt != None:\n",
    "            lang = ftlangdetect.detect(re_txt.group(), low_memory=False)['lang']\n",
    "            if lang == 'jp':\n",
    "                lst = clean_jp(lst)\n",
    "            elif lang == 'zh':\n",
    "                lst = clean_zh(lst)\n",
    "        else:\n",
    "            lang = ftlangdetect.detect(txt, low_memory=False)['lang']\n",
    "            if lang not in ['en','de','zh','fr','es','ru','it','Nothing']:\n",
    "                lang = 'Other'\n",
    "    return lst, lang\n",
    "    \n",
    "def is_zh(in_str):\n",
    "    \"\"\"\n",
    "    >>> is_zh(u'おはよう')\n",
    "    False\n",
    "    >>> is_zh(u'&#35828;地')\n",
    "    True\n",
    "    \"\"\"\n",
    "    questions_before = [s for s in in_str]\n",
    "    questions_gb2312 = [s for s in \\\n",
    "        in_str.encode('gb2312','ignore').decode('gb2312')]\n",
    "    questions_cp932 = [s for s in \\\n",
    "        in_str.encode('cp932','ignore').decode('cp932')]\n",
    "    if (questions_gb2312 == questions_before) and (\n",
    "        (set(questions_before) - set(questions_cp932)) != set([])):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf557d06-c254-4757-8313-d7a9c9b1bb9a",
   "metadata": {},
   "source": [
    "# pid_to_info_all.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ddc6e3-23ed-4039-a8e2-def8f512a206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"../raw/pid_to_info_all.json\"\n",
    "df = pd.read_json(path)\n",
    "df = df.T\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb7de040-110c-45e6-83a5-62d3c6fefc1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317302it [34:54, 151.49it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = []\n",
    "for index, row in tqdm.tqdm(df.iterrows()):\n",
    "    p_id = row['id']\n",
    "    # 前処理&リスト化&日本語・中国語対応：title\n",
    "    row['title'] = clean_text(row['title'])\n",
    "    title = split_text(row['title'])\n",
    "    title, t_lang = judge_lang(row['title'], title)\n",
    "    \n",
    "    # 前処理&リスト化&日本語・中国語対応：abstract\n",
    "    row['abstract'] = clean_text(row['abstract'])\n",
    "    abstract = split_text(row['abstract'])\n",
    "    abstract, a_lang = judge_lang(row['abstract'], abstract) \n",
    "    \n",
    "    # 前処理&リスト化&日本語・中国語対応：keywords\n",
    "    row['keywords'] = list(itertools.chain.from_iterable(\n",
    "        [clean_text(key).strip().split(\" \") for key in row['keywords']]))\n",
    "    keywords = split_list(row['keywords'])\n",
    "    keywords, k_lang = judge_lang(' '.join(row['keywords']), keywords)\n",
    "    \n",
    "    # 前処理&リスト化&日本語・中国語対応：venue\n",
    "    row['venue'] = clean_text(row['venue'])\n",
    "    venue = split_text(row['venue'])\n",
    "    venue, v_lang = judge_lang(row['venue'], venue)\n",
    "\n",
    "    #前処理&リスト化・著者数:authors\n",
    "    authors,names,orgs = clean_authors(row['authors'])\n",
    "    num_authors = len(list(itertools.chain.from_iterable([a[0] for a in authors if a[0] != ''])))\n",
    "    #前処理:year\n",
    "    year = clean_year(row['year'])\n",
    "    \n",
    "    cleaned_data.append([\n",
    "        row['id'],\n",
    "        title,\n",
    "        authors,\n",
    "        abstract,\n",
    "        keywords,\n",
    "        venue,\n",
    "        year,\n",
    "        names,\n",
    "        orgs,\n",
    "        num_authors,\n",
    "        t_lang,\n",
    "        a_lang,\n",
    "        k_lang,\n",
    "        v_lang\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4c22de5-a093-48a5-9b50-98e7a57d0168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>title</th><th>authors</th><th>abstract</th><th>keywords</th><th>venue</th><th>year</th><th>names</th><th>orgs</th><th>num_authors</th><th>t_lang</th><th>a_lang</th><th>k_lang</th><th>v_lang</th></tr><tr><td>str</td><td>list[str]</td><td>list[list[str]]</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i64</td><td>list[str]</td><td>list[str]</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;6IsfnuWU&quot;</td><td>[&quot;probabilist&quot;, &quot;skylin&quot;, … &quot;window&quot;]</td><td>[[&quot;wenjiezhang&quot;, &quot;sydney,unsw&quot;], [&quot;linxuemin&quot;, &quot;sydney,unsw&quot;], … [&quot;jeffreyxuyu&quot;, &quot;hong,kong&quot;]]</td><td>[&quot;skylin&quot;, &quot;comput&quot;, … &quot;time&quot;]</td><td>[&quot;continu&quot;, &quot;skylin&quot;, … &quot;window&quot;]</td><td>[&quot;icd&quot;, &quot;proceed&quot;, … &quot;engin&quot;]</td><td>2009</td><td>[&quot;yingzhang&quot;, &quot;linxuemin&quot;, … &quot;wangwei&quot;]</td><td>[&quot;sydney&quot;, &quot;hong&quot;, … &quot;kong&quot;]</td><td>47</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td></tr><tr><td>&quot;8B8GhlnI&quot;</td><td>[&quot;editori&quot;, &quot;knowledg&quot;, … &quot;environ&quot;]</td><td>[[&quot;chenliming&quot;, &quot;NULL&quot;], [&quot;chrisnugent&quot;, &quot;comput&quot;], … [&quot;yuzhiwen&quot;, &quot;NULL&quot;]]</td><td>[]</td><td>[&quot;activ&quot;, &quot;recognit&quot;]</td><td>[&quot;period&quot;]</td><td>2011</td><td>[&quot;yuzhiwen&quot;, &quot;chenliming&quot;, … &quot;cookdiane&quot;]</td><td>[&quot;comput&quot;, &quot;NULL&quot;]</td><td>38</td><td>&quot;en&quot;</td><td>&quot;Nothing&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td></tr><tr><td>&quot;4dZKGwVR&quot;</td><td>[&quot;subscrib&quot;, &quot;assign&quot;, … &quot;subscrib&quot;]</td><td>[[&quot;albertyu&quot;, &quot;sci,comp,usa,durham,nc,duke&quot;], [&quot;agarwalkpankaj&quot;, &quot;sci,comp,usa,durham,nc,duke&quot;], [&quot;junyang&quot;, &quot;sci,comp,usa,durham,nc,duke&quot;]]</td><td>[&quot;studi&quot;, &quot;problem&quot;, … &quot;subscrib&quot;]</td><td>[&quot;mont&quot;, &quot;carlo&quot;, … &quot;assign&quot;]</td><td>[&quot;icd&quot;, &quot;proceed&quot;, … &quot;engin&quot;]</td><td>2011</td><td>[&quot;agarwalkpankaj&quot;, &quot;junyang&quot;, &quot;albertyu&quot;]</td><td>[&quot;sci&quot;, &quot;comp&quot;, … &quot;duke&quot;]</td><td>29</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td></tr><tr><td>&quot;V1JgT3OM&quot;</td><td>[&quot;tree&quot;, &quot;mine&quot;, … &quot;meet&quot;]</td><td>[[&quot;yuzhiwen&quot;, &quot;northwestern,xi,polytechn&quot;], [&quot;yuzhiyong&quot;, &quot;fuzhou&quot;], … [&quot;nakamurayuichi&quot;, &quot;kyoto&quot;]]</td><td>[&quot;abstractdiscov&quot;, &quot;semant&quot;, … &quot;interact&quot;]</td><td>[&quot;discov&quot;, &quot;pattern&quot;, … &quot;analysi&quot;]</td><td>[&quot;period&quot;]</td><td>2012</td><td>[&quot;beckerchristian&quot;, &quot;nakamurayuichi&quot;, … &quot;xingshezhou&quot;]</td><td>[&quot;mannheim&quot;, &quot;northwestern&quot;, … &quot;kyoto&quot;]</td><td>57</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td></tr><tr><td>&quot;HMvrPr2W&quot;</td><td>[&quot;protein&quot;, &quot;function&quot;, … &quot;classif&quot;]</td><td>[[&quot;guoxianyu&quot;, &quot;technolog,south,guangzhou,southwest,beibei&quot;], [&quot;huzefarangwala&quot;, &quot;georg,fairfax,mason&quot;], … [&quot;yuzhiwen&quot;, &quot;south,technolog,guangzhou&quot;]]</td><td>[&quot;abstracthigh&quot;, &quot;throughput&quot;, … &quot;kernel&quot;]</td><td>[&quot;heterogen&quot;, &quot;proteom&quot;, … &quot;vector&quot;]</td><td>[&quot;ieee&quot;, &quot;acm&quot;, … &quot;bioinformat&quot;]</td><td>2013</td><td>[&quot;carlottadomeniconi&quot;, &quot;guojizhang&quot;, … &quot;yuzhiwen&quot;]</td><td>[&quot;technolog&quot;, &quot;south&quot;, … &quot;beibei&quot;]</td><td>59</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td><td>&quot;en&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 14)\n",
       "┌──────────┬───────────────┬───────────────┬──────────────┬───┬────────┬─────────┬────────┬────────┐\n",
       "│ id       ┆ title         ┆ authors       ┆ abstract     ┆ … ┆ t_lang ┆ a_lang  ┆ k_lang ┆ v_lang │\n",
       "│ ---      ┆ ---           ┆ ---           ┆ ---          ┆   ┆ ---    ┆ ---     ┆ ---    ┆ ---    │\n",
       "│ str      ┆ list[str]     ┆ list[list[str ┆ list[str]    ┆   ┆ str    ┆ str     ┆ str    ┆ str    │\n",
       "│          ┆               ┆ ]]            ┆              ┆   ┆        ┆         ┆        ┆        │\n",
       "╞══════════╪═══════════════╪═══════════════╪══════════════╪═══╪════════╪═════════╪════════╪════════╡\n",
       "│ 6IsfnuWU ┆ [\"probabilist ┆ [[\"wenjiezhan ┆ [\"skylin\",   ┆ … ┆ en     ┆ en      ┆ en     ┆ en     │\n",
       "│          ┆ \", \"skylin\",  ┆ g\", \"sydney,u ┆ \"comput\", …  ┆   ┆        ┆         ┆        ┆        │\n",
       "│          ┆ … \"w…         ┆ nsw\"…         ┆ \"time\"]      ┆   ┆        ┆         ┆        ┆        │\n",
       "│ 8B8GhlnI ┆ [\"editori\",   ┆ [[\"chenliming ┆ []           ┆ … ┆ en     ┆ Nothing ┆ en     ┆ en     │\n",
       "│          ┆ \"knowledg\", … ┆ \", \"NULL\"],   ┆              ┆   ┆        ┆         ┆        ┆        │\n",
       "│          ┆ \"env…         ┆ [\"chr…        ┆              ┆   ┆        ┆         ┆        ┆        │\n",
       "│ 4dZKGwVR ┆ [\"subscrib\",  ┆ [[\"albertyu\", ┆ [\"studi\",    ┆ … ┆ en     ┆ en      ┆ en     ┆ en     │\n",
       "│          ┆ \"assign\", …   ┆ \"sci,comp,usa ┆ \"problem\", … ┆   ┆        ┆         ┆        ┆        │\n",
       "│          ┆ \"subs…        ┆ ,du…          ┆ \"subscr…     ┆   ┆        ┆         ┆        ┆        │\n",
       "│ V1JgT3OM ┆ [\"tree\",      ┆ [[\"yuzhiwen\", ┆ [\"abstractdi ┆ … ┆ en     ┆ en      ┆ en     ┆ en     │\n",
       "│          ┆ \"mine\", …     ┆ \"northwestern ┆ scov\",       ┆   ┆        ┆         ┆        ┆        │\n",
       "│          ┆ \"meet\"]       ┆ ,xi…          ┆ \"semant\", …… ┆   ┆        ┆         ┆        ┆        │\n",
       "│ HMvrPr2W ┆ [\"protein\",   ┆ [[\"guoxianyu\" ┆ [\"abstracthi ┆ … ┆ en     ┆ en      ┆ en     ┆ en     │\n",
       "│          ┆ \"function\", … ┆ , \"technolog, ┆ gh\", \"throug ┆   ┆        ┆         ┆        ┆        │\n",
       "│          ┆ \"cla…         ┆ sout…         ┆ hput\",…      ┆   ┆        ┆         ┆        ┆        │\n",
       "└──────────┴───────────────┴───────────────┴──────────────┴───┴────────┴─────────┴────────┴────────┘"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix = pl.DataFrame(cleaned_data, schema=['id','title','authors','abstract','keywords','venue','year','names','orgs','num_authors','t_lang','a_lang','k_lang', 'v_lang'])\n",
    "df_fix.write_parquet('../test_data/cleaned_pid_to_info_all_v6.parquet')\n",
    "df_fix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9788662-22a5-4078-8d2a-0bc180632325",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "34d1bd4f-ba59-43f2-a048-3ee6f87d21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317302, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>names</th>\n",
       "      <th>orgs</th>\n",
       "      <th>num_authors</th>\n",
       "      <th>t_lang</th>\n",
       "      <th>a_lang</th>\n",
       "      <th>k_lang</th>\n",
       "      <th>v_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6IsfnuWU</td>\n",
       "      <td>[probabilist, skylin, oper, slide, window]</td>\n",
       "      <td>[[wenjiezhang, sydney,unsw], [linxuemin, sydne...</td>\n",
       "      <td>[skylin, comput, mani, applic, includ, multi, ...</td>\n",
       "      <td>[continu, skylin, queri, probabilist, skylin, ...</td>\n",
       "      <td>[icd, proceed, ieee, intern, confer, data, engin]</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>[yingzhang, linxuemin, jeffreyxuyu, wenjiezhan...</td>\n",
       "      <td>[sydney, hong, unsw, kong]</td>\n",
       "      <td>47</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8B8GhlnI</td>\n",
       "      <td>[editori, knowledg, driven, activ, recognit, i...</td>\n",
       "      <td>[[chenliming, NULL], [chrisnugent, comput], [c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[activ, recognit]</td>\n",
       "      <td>[period]</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>[yuzhiwen, chenliming, chrisnugent, cookdiane]</td>\n",
       "      <td>[comput, NULL]</td>\n",
       "      <td>38</td>\n",
       "      <td>en</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  6IsfnuWU         [probabilist, skylin, oper, slide, window]   \n",
       "1  8B8GhlnI  [editori, knowledg, driven, activ, recognit, i...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [[wenjiezhang, sydney,unsw], [linxuemin, sydne...   \n",
       "1  [[chenliming, NULL], [chrisnugent, comput], [c...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  [skylin, comput, mani, applic, includ, multi, ...   \n",
       "1                                                 []   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [continu, skylin, queri, probabilist, skylin, ...   \n",
       "1                                  [activ, recognit]   \n",
       "\n",
       "                                               venue    year  \\\n",
       "0  [icd, proceed, ieee, intern, confer, data, engin]  2009.0   \n",
       "1                                           [period]  2011.0   \n",
       "\n",
       "                                               names  \\\n",
       "0  [yingzhang, linxuemin, jeffreyxuyu, wenjiezhan...   \n",
       "1     [yuzhiwen, chenliming, chrisnugent, cookdiane]   \n",
       "\n",
       "                         orgs  num_authors t_lang   a_lang k_lang v_lang  \n",
       "0  [sydney, hong, unsw, kong]           47     en       en     en     en  \n",
       "1              [comput, NULL]           38     en  Nothing     en     en  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fix = pd.read_parquet('../test_data/cleaned_pid_to_info_all_v6.parquet')\n",
    "print(df_fix.shape)\n",
    "df_fix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "839187fa-2c49-423c-9ec0-be8b27f9ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317302it [00:42, 7437.87it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for index, row in tqdm.tqdm(df_fix.iterrows()):\n",
    "    temp_corpus = []\n",
    "    temp_corpus.extend(row['title'])\n",
    "    temp_corpus.extend(row['abstract'])\n",
    "    temp_corpus.extend(row['keywords'])\n",
    "    if row['venue'] is not None:\n",
    "        temp_corpus.extend(row['venue'])\n",
    "    if row['year'] > 0: \n",
    "        temp_corpus.extend([str(int(row['year']))])\n",
    "    if len(temp_corpus) > 1:\n",
    "        corpus.append(temp_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58bd8306-010f-45e1-84ec-67aabb79cfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 14223800.0\n",
      "Loss after epoch 2: 10850790.0\n",
      "Loss after epoch 3: 9550474.0\n",
      "Loss after epoch 4: 6575896.0\n",
      "Loss after epoch 5: 6562656.0\n",
      "Loss after epoch 6: 6587584.0\n",
      "Loss after epoch 7: 6593000.0\n",
      "Loss after epoch 8: 6210856.0\n",
      "Loss after epoch 9: 1746176.0\n",
      "Loss after epoch 10: 1714200.0\n",
      "Loss after epoch 11: 1691856.0\n",
      "Loss after epoch 12: 1674488.0\n",
      "Loss after epoch 13: 1641664.0\n",
      "Loss after epoch 14: 1624768.0\n",
      "Loss after epoch 15: 1576440.0\n",
      "Loss after epoch 16: 1582600.0\n",
      "Loss after epoch 17: 1549176.0\n",
      "Loss after epoch 18: 1520848.0\n",
      "Loss after epoch 19: 1484792.0\n",
      "Loss after epoch 20: 1443536.0\n",
      "Loss after epoch 21: 1413440.0\n",
      "Loss after epoch 22: 1341088.0\n",
      "Loss after epoch 23: 1303208.0\n",
      "Loss after epoch 24: 1266968.0\n",
      "Loss after epoch 25: 1211408.0\n",
      "Loss after epoch 26: 1145200.0\n",
      "Loss after epoch 27: 1066416.0\n",
      "Loss after epoch 28: 981016.0\n",
      "Loss after epoch 29: 888112.0\n",
      "Loss after epoch 30: 814536.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('graph', 0.5929703116416931),\n",
       " ('commun', 0.5862277150154114),\n",
       " ('connect', 0.5703359246253967),\n",
       " ('architectur', 0.5659001469612122),\n",
       " ('traffic', 0.5612089037895203),\n",
       " ('node', 0.5335066318511963),\n",
       " ('system', 0.5230070948600769),\n",
       " ('internet', 0.5210995078086853),\n",
       " ('link', 0.517693281173706),\n",
       " ('rout', 0.4998023509979248)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LossLogger(CallbackAny2Vec):\n",
    "    '''Callback to log loss after each epoch.'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_previous_step = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_previous_step\n",
    "        self.loss_previous_step = loss\n",
    "        self.epoch += 1\n",
    "        print(f\"Loss after epoch {self.epoch}: {loss_now}\")\n",
    "\n",
    "model = word2vec.Word2Vec(corpus, vector_size=128, min_count=2, window=10, negative=5, epochs=30, sg=0, \n",
    "                          compute_loss=True,callbacks=[LossLogger()]\n",
    "                         )\n",
    "model.save('../test_data/w2v_concat_cbow_128dim_min2_window10_neg5_epoch30_v6.bin')\n",
    "model.wv.most_similar('network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "80cb83c0-77db-4f86-9de9-303c3bc896cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317302it [00:27, 11620.45it/s]\n"
     ]
    }
   ],
   "source": [
    "author_corpus = []\n",
    "for index, row in tqdm.tqdm(df_fix.iterrows()):\n",
    "    for r in row['authors']:\n",
    "        orgs = r[1].split(',')\n",
    "        if orgs != ['NULL'] and len(orgs)>1:\n",
    "            author_corpus.append(orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b1c12145-02a5-4f55-8124-e08d784f340d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('siti', 0.7422759532928467),\n",
       " ('wuahan', 0.6284927129745483),\n",
       " ('acadami', 0.3536508083343506),\n",
       " ('haidian', 0.34424933791160583),\n",
       " ('neural', 0.33910518884658813),\n",
       " ('hung', 0.3367649018764496),\n",
       " ('eal', 0.3324764668941498),\n",
       " ('safti', 0.31024035811424255),\n",
       " ('hongshan', 0.30843672156333923),\n",
       " ('centenari', 0.28941377997398376)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_model = word2vec.Word2Vec(author_corpus, vector_size=128, min_count=5, window=5, negative=5, epochs=30, sg=0)\n",
    "o_model.save('../test_data/w2v_org_cbow_128dim_window5_min5_neg5_epoch30_v6.bin')\n",
    "o_model.wv.most_similar('univ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6caf78-4c79-4531-b177-11b3e34a305a",
   "metadata": {},
   "source": [
    "# train_author.json/ind_valid_author.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa44cea-1bbc-4894-8a99-5b92fbb55846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>normal_data</th>\n",
       "      <th>outliers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iki037dt</td>\n",
       "      <td>atsushi ochiai</td>\n",
       "      <td>[YzOCpPTO, AblgcGjH, B5aouLse, u1G7wBEv, W7w6P...</td>\n",
       "      <td>[XL3wd3CP, BTKTiJp2, JxSjl5xc, 0jyMLgRt, uHWx8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZihzMro7</td>\n",
       "      <td>mingwu yang</td>\n",
       "      <td>[C58t0yYu, sWIRnfR3, HJW8h2mo, 0Ptx4O5n, fU4vB...</td>\n",
       "      <td>[qK8llKzD, I0eTdaAG, nFebDDiR, 903CyaNQ, Q45WM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WXMYBk3c</td>\n",
       "      <td>jianzhao huang</td>\n",
       "      <td>[lJAIOXO4, fYJcce0K, ZaeOFAcI, kg9xDSXm, T37S3...</td>\n",
       "      <td>[HwaUxOes, nvELwvhl, 6Z6SRTQh, R1yeZqOY, qnwco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WrCODHhe</td>\n",
       "      <td>xuebiao yao</td>\n",
       "      <td>[3fYoJb1W, wjt8Y8ho, pPx6o7KZ, xgRarLPn, 9w9yz...</td>\n",
       "      <td>[OtmIuFFb, wnP8OmXf, IZ1qVc9S, YccNQrlZ, sLO7c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k3uSCGEE</td>\n",
       "      <td>shunlin tang</td>\n",
       "      <td>[gTeQer76, mVk2vmmN, TLKSll8D, Eg5NcmZ2, kM5Ip...</td>\n",
       "      <td>[xPmu4CGB, buwfccml, fBPzgpof, HgjM9QKW, rPk5S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            name  \\\n",
       "0  Iki037dt  atsushi ochiai   \n",
       "1  ZihzMro7     mingwu yang   \n",
       "2  WXMYBk3c  jianzhao huang   \n",
       "3  WrCODHhe     xuebiao yao   \n",
       "4  k3uSCGEE    shunlin tang   \n",
       "\n",
       "                                         normal_data  \\\n",
       "0  [YzOCpPTO, AblgcGjH, B5aouLse, u1G7wBEv, W7w6P...   \n",
       "1  [C58t0yYu, sWIRnfR3, HJW8h2mo, 0Ptx4O5n, fU4vB...   \n",
       "2  [lJAIOXO4, fYJcce0K, ZaeOFAcI, kg9xDSXm, T37S3...   \n",
       "3  [3fYoJb1W, wjt8Y8ho, pPx6o7KZ, xgRarLPn, 9w9yz...   \n",
       "4  [gTeQer76, mVk2vmmN, TLKSll8D, Eg5NcmZ2, kM5Ip...   \n",
       "\n",
       "                                            outliers  \n",
       "0  [XL3wd3CP, BTKTiJp2, JxSjl5xc, 0jyMLgRt, uHWx8...  \n",
       "1  [qK8llKzD, I0eTdaAG, nFebDDiR, 903CyaNQ, Q45WM...  \n",
       "2  [HwaUxOes, nvELwvhl, 6Z6SRTQh, R1yeZqOY, qnwco...  \n",
       "3  [OtmIuFFb, wnP8OmXf, IZ1qVc9S, YccNQrlZ, sLO7c...  \n",
       "4  [xPmu4CGB, buwfccml, fBPzgpof, HgjM9QKW, rPk5S...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(515, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>papers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fkb16wn7</td>\n",
       "      <td>jitendra malik</td>\n",
       "      <td>[0DchSY2n, 0Gw6iDes, 0PgoDSAP, 0S7g2B2l, 0YJjx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KKiBE172</td>\n",
       "      <td>hongbo xin</td>\n",
       "      <td>[0ezX6FSp, 3iqa5cb0, 4N47MJgM, 4oIu3mlO, 4tXA4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grbM72Lg</td>\n",
       "      <td>junjie mao</td>\n",
       "      <td>[0fE5PVDQ, 13lxiUDV, 1K5HU0Iy, 20BOekkh, 2d0LD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xrp9GO54</td>\n",
       "      <td>xiaoxia wan</td>\n",
       "      <td>[2YBSYv1q, 3IOmTqw5, 60oLEMcv, 7VGBEWQB, 7xeeg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9Gs8Wj3Y</td>\n",
       "      <td>tie gang</td>\n",
       "      <td>[05R8WTSY, 06xOWmCZ, 0Ac7puFw, 0HUElSew, 0HpAa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            name                                             papers\n",
       "0  Fkb16wn7  jitendra malik  [0DchSY2n, 0Gw6iDes, 0PgoDSAP, 0S7g2B2l, 0YJjx...\n",
       "1  KKiBE172      hongbo xin  [0ezX6FSp, 3iqa5cb0, 4N47MJgM, 4oIu3mlO, 4tXA4...\n",
       "2  grbM72Lg      junjie mao  [0fE5PVDQ, 13lxiUDV, 1K5HU0Iy, 20BOekkh, 2d0LD...\n",
       "3  Xrp9GO54     xiaoxia wan  [2YBSYv1q, 3IOmTqw5, 60oLEMcv, 7VGBEWQB, 7xeeg...\n",
       "4  9Gs8Wj3Y        tie gang  [05R8WTSY, 06xOWmCZ, 0Ac7puFw, 0HUElSew, 0HpAa..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_org = pd.read_json('../raw/train_author.json')\n",
    "df_train_org = df_train_org.T\n",
    "df_train_org = df_train_org.reset_index()\n",
    "df_train_org.columns = ['id', 'name', 'normal_data', 'outliers']\n",
    "\n",
    "df_test_org = pd.read_json('../raw/ind_test_author_filter_public.json')\n",
    "df_test_org = df_test_org.T\n",
    "df_test_org = df_test_org.reset_index()\n",
    "df_test_org.columns = ['id', 'name', 'papers']\n",
    "print(df_train_org.shape)\n",
    "display(df_train_org.head())\n",
    "print(df_test_org.shape)\n",
    "display(df_test_org.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f80de01-4fed-4655-ad37-940320cd228f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>names</th>\n",
       "      <th>orgs</th>\n",
       "      <th>num_authors</th>\n",
       "      <th>t_lang</th>\n",
       "      <th>a_lang</th>\n",
       "      <th>k_lang</th>\n",
       "      <th>v_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6IsfnuWU</td>\n",
       "      <td>[probabilist, skylin, oper, slide, window]</td>\n",
       "      <td>[[wenjiezhang, sydney,unsw], [linxuemin, sydne...</td>\n",
       "      <td>[skylin, comput, mani, applic, includ, multi, ...</td>\n",
       "      <td>[continu, skylin, queri, probabilist, skylin, ...</td>\n",
       "      <td>[icd, proceed, ieee, intern, confer, data, engin]</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>[yingzhang, linxuemin, jeffreyxuyu, wenjiezhan...</td>\n",
       "      <td>[sydney, hong, unsw, kong]</td>\n",
       "      <td>47</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8B8GhlnI</td>\n",
       "      <td>[editori, knowledg, driven, activ, recognit, i...</td>\n",
       "      <td>[[chenliming, NULL], [chrisnugent, comput], [c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[activ, recognit]</td>\n",
       "      <td>[period]</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>[yuzhiwen, chenliming, chrisnugent, cookdiane]</td>\n",
       "      <td>[comput, NULL]</td>\n",
       "      <td>38</td>\n",
       "      <td>en</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4dZKGwVR</td>\n",
       "      <td>[subscrib, assign, wide, area, content, publis...</td>\n",
       "      <td>[[albertyu, sci,comp,usa,durham,nc,duke], [aga...</td>\n",
       "      <td>[studi, problem, assign, subscrib, broker, wid...</td>\n",
       "      <td>[mont, carlo, approxim, algorithm, futur, eval...</td>\n",
       "      <td>[icd, proceed, ieee, 0th, intern, confer, data...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>[agarwalkpankaj, junyang, albertyu]</td>\n",
       "      <td>[sci, comp, usa, durham, nc, duke]</td>\n",
       "      <td>29</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1JgT3OM</td>\n",
       "      <td>[tree, mine, discov, pattern, human, interact,...</td>\n",
       "      <td>[[yuzhiwen, northwestern,xi,polytechn], [yuzhi...</td>\n",
       "      <td>[abstractdiscov, semant, knowledg, signific, u...</td>\n",
       "      <td>[discov, pattern, interact, flow, pattern, tre...</td>\n",
       "      <td>[period]</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>[beckerchristian, nakamurayuichi, yuzhiyong, y...</td>\n",
       "      <td>[mannheim, northwestern, xi, polytechn, fuzhou...</td>\n",
       "      <td>57</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HMvrPr2W</td>\n",
       "      <td>[protein, function, predict, multi, label, ens...</td>\n",
       "      <td>[[guoxianyu, technolog,south,guangzhou,southwe...</td>\n",
       "      <td>[abstracthigh, throughput, experiment, techniq...</td>\n",
       "      <td>[heterogen, proteom, data, set, multilabel, en...</td>\n",
       "      <td>[ieee, acm, transact, comput, biolog, bioinfor...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>[carlottadomeniconi, guojizhang, guoxianyu, hu...</td>\n",
       "      <td>[technolog, south, guangzhou, mason, southwest...</td>\n",
       "      <td>59</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  6IsfnuWU         [probabilist, skylin, oper, slide, window]   \n",
       "1  8B8GhlnI  [editori, knowledg, driven, activ, recognit, i...   \n",
       "2  4dZKGwVR  [subscrib, assign, wide, area, content, publis...   \n",
       "3  V1JgT3OM  [tree, mine, discov, pattern, human, interact,...   \n",
       "4  HMvrPr2W  [protein, function, predict, multi, label, ens...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [[wenjiezhang, sydney,unsw], [linxuemin, sydne...   \n",
       "1  [[chenliming, NULL], [chrisnugent, comput], [c...   \n",
       "2  [[albertyu, sci,comp,usa,durham,nc,duke], [aga...   \n",
       "3  [[yuzhiwen, northwestern,xi,polytechn], [yuzhi...   \n",
       "4  [[guoxianyu, technolog,south,guangzhou,southwe...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  [skylin, comput, mani, applic, includ, multi, ...   \n",
       "1                                                 []   \n",
       "2  [studi, problem, assign, subscrib, broker, wid...   \n",
       "3  [abstractdiscov, semant, knowledg, signific, u...   \n",
       "4  [abstracthigh, throughput, experiment, techniq...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [continu, skylin, queri, probabilist, skylin, ...   \n",
       "1                                  [activ, recognit]   \n",
       "2  [mont, carlo, approxim, algorithm, futur, eval...   \n",
       "3  [discov, pattern, interact, flow, pattern, tre...   \n",
       "4  [heterogen, proteom, data, set, multilabel, en...   \n",
       "\n",
       "                                               venue    year  \\\n",
       "0  [icd, proceed, ieee, intern, confer, data, engin]  2009.0   \n",
       "1                                           [period]  2011.0   \n",
       "2  [icd, proceed, ieee, 0th, intern, confer, data...  2011.0   \n",
       "3                                           [period]  2012.0   \n",
       "4  [ieee, acm, transact, comput, biolog, bioinfor...  2013.0   \n",
       "\n",
       "                                               names  \\\n",
       "0  [yingzhang, linxuemin, jeffreyxuyu, wenjiezhan...   \n",
       "1     [yuzhiwen, chenliming, chrisnugent, cookdiane]   \n",
       "2                [agarwalkpankaj, junyang, albertyu]   \n",
       "3  [beckerchristian, nakamurayuichi, yuzhiyong, y...   \n",
       "4  [carlottadomeniconi, guojizhang, guoxianyu, hu...   \n",
       "\n",
       "                                                orgs  num_authors t_lang  \\\n",
       "0                         [sydney, hong, unsw, kong]           47     en   \n",
       "1                                     [comput, NULL]           38     en   \n",
       "2                 [sci, comp, usa, durham, nc, duke]           29     en   \n",
       "3  [mannheim, northwestern, xi, polytechn, fuzhou...           57     en   \n",
       "4  [technolog, south, guangzhou, mason, southwest...           59     en   \n",
       "\n",
       "    a_lang k_lang v_lang  \n",
       "0       en     en     en  \n",
       "1  Nothing     en     en  \n",
       "2       en     en     en  \n",
       "3       en     en     en  \n",
       "4       en     en     en  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../test_data/cleaned_pid_to_info_all_v6.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9581387b-c73a-421d-b3c9-aeeb3607a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "    name = clean_text(name)\n",
    "    name = name.split(' ')\n",
    "    name = [n for n in name if n.isdigit() != True]\n",
    "    if len(name) == 2:\n",
    "        name = [\n",
    "            name[0] + name[1],\n",
    "            # name[0][0] + '_' + name[1],\n",
    "            name[1] + name[0],\n",
    "            # name[1][0] + '_' + name[0]\n",
    "        ]\n",
    "        name.sort()\n",
    "        return name[0]\n",
    "    elif len(name) > 2:\n",
    "        name = [\n",
    "            name[0] + name[1] + name[2],\n",
    "            # name[0][0] + '_' + name[1][0] + '_' + name[2],\n",
    "            name[2] + name[1] + name[0],\n",
    "            # name[2][0] + '_' + name[1][0] + '_' + name[0],\n",
    "            # name[0][0] + '_' + name[1][0] + name[2],\n",
    "            # name[2][0] + '_' + name[1][0] + name[0],\n",
    "            # name[0] + '_'  + name[2],\n",
    "            # name[0][0] + '_'  + name[2],\n",
    "            # name[2] + '_' + name[0],\n",
    "            # name[2][0] + '_' + name[0],\n",
    "        ]\n",
    "        name.sort()\n",
    "        return name[0]\n",
    "    elif len(name) == 1 and name != ['']:\n",
    "        return name[0]\n",
    "    else:\n",
    "        return 'NULL'\n",
    "train_list,test_list = [],[]\n",
    "for index, row in df_train_org.iterrows():\n",
    "    row['name'] = clean_name(row['name'])\n",
    "    train_list.append(row)\n",
    "for index, row in df_test_org.iterrows():\n",
    "    row['name'] = clean_name(row['name'])\n",
    "    test_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e4e1e4-2192-45db-adb0-6bc76d1a5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pl.from_pandas(pd.DataFrame(train_list))\n",
    "train.write_parquet('../test_data/train_author.parquet')\n",
    "test = pl.from_pandas(pd.DataFrame(test_list))\n",
    "test.write_parquet('../test_data/ind_test_author_filter_public.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cb792-831f-487c-a947-264b7b4ae99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
