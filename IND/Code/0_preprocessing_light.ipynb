{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0da8df7-25e8-4484-a569-bf8cf7d25412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import tqdm\n",
    "import jieba\n",
    "import regex\n",
    "import neologdn\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import ftlangdetect\n",
    "from janome.tokenizer import Tokenizer\n",
    "from pypinyin import lazy_pinyin\n",
    "from pykakasi import kakasi\n",
    "\n",
    "os.makedirs(\"../test_data/\", exist_ok=True)\n",
    "\n",
    "kks = kakasi()\n",
    "ps =PorterStemmer()\n",
    "tk = Tokenizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = ['at', 'based', 'in', 'of', 'for', 'on', 'and', 'to', 'an', 'using', 'with', 'the', 'by', 'we', 'be', 'is', 'are', 'can'] + stopwords.words('english')+ stopwords.words('spanish')+ stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c186f96-786f-4382-b8e4-4edef88e20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_list = []\n",
    "def clean_text(txt):\n",
    "    if txt != None:\n",
    "        puncs = '[!�“”\"#$%&\\'()【】（）／《》\\・*+,-./–:;<=>?@[\\\\]^_`{|}~—～’、。]+'\n",
    "        txt = txt.strip()\n",
    "        txt = txt.lower()\n",
    "        txt = txt.replace('\\n', ' ')\n",
    "        txt = txt.replace(u\"\\xa0\", u\" \")\n",
    "        txt = txt.replace('\\\\',' ')\n",
    "        txt = txt.replace('‐',' ')\n",
    "        txt = re.sub(puncs, ' ', txt)\n",
    "        txt = re.sub(r'\\s{2,}', ' ', txt).strip()\n",
    "        return txt\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def split_text(txt):\n",
    "    if txt == 'null' or txt == '':\n",
    "        return []\n",
    "    else:\n",
    "        txt = txt.split(' ')\n",
    "        txt = [re.sub(r'\\d+', '0', word) for word in txt]\n",
    "        return txt\n",
    "\n",
    "def split_list(lst):\n",
    "    if lst == ['null']:\n",
    "        return []\n",
    "    else:\n",
    "        lst = [re.sub(r'\\d+', '0', word) for word in lst]\n",
    "        return lst\n",
    "    \n",
    "def clean_authors(authors):\n",
    "    cleaned_authors, names_list, orgs_list = [], [], []\n",
    "    if len(authors) > 0:\n",
    "        for author in authors:\n",
    "            name = author.get('name')\n",
    "            if name != '':\n",
    "                name = clean_text(name)\n",
    "                ptn = r'[\\p{Block=Hiragana}\\p{Script=Katakana}\\p{Han}\\p{Script_Extensions=Han}\\u2E80-\\u2FDF\\u3005-\\u3007\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF\\U00020000-\\U0002EBEF]+'\n",
    "                re_ptn = regex.compile(ptn)\n",
    "                # 漢字・ひらがな・カタカナが存在する場合 =>日本人or中国人？\n",
    "                if re_ptn.search(name):\n",
    "                    # 英語削除（漢字 + ピンイン表記の入力が多い・どちらもフルネーム）\n",
    "                    re_txt = re_ptn.finditer(name)\n",
    "                    temp = []\n",
    "                    for t in re_txt:\n",
    "                        temp.append(t.group())\n",
    "                    name = ' '.join(temp)\n",
    "                    check_list.append(name)\n",
    "                    # jp or zh\n",
    "                    lang = ftlangdetect.detect(name, low_memory=False)['lang']\n",
    "                    if lang == 'zh':\n",
    "                        name = name.removesuffix(' 著')\n",
    "                        # 複数人の入力がある場合の対処\n",
    "                        if len(name) > 4 and len(name.split(' ')) > 2:\n",
    "                            tmp = []\n",
    "                            for n in name.split(' '):\n",
    "                                n = ' '.join(lazy_pinyin(n))\n",
    "                                n = re.sub(r'\\s{2,}', ' ', n).strip()\n",
    "                                n = split_name(n)\n",
    "                                tmp.extend(n)\n",
    "                            name = tmp\n",
    "                        else:            \n",
    "                            name = ' '.join(lazy_pinyin(name))\n",
    "                            name = re.sub(r'\\s{2,}', ' ', name).strip()\n",
    "                            name = split_name(name)\n",
    "                    # jaは精度甘め\n",
    "                    elif lang == 'ja':\n",
    "                        if is_zh(name) or len(name) < 4:\n",
    "                            name = ' '.join(lazy_pinyin(name))\n",
    "                            name = re.sub(r'\\s{2,}', ' ', name).strip()\n",
    "                            name = split_name(name)\n",
    "                        else:\n",
    "                            # ローマ字表記変換\n",
    "                            jps = []\n",
    "                            result = kks.convert(name)\n",
    "                            for w in result:\n",
    "                                if w['hepburn'] != ' ':\n",
    "                                    jps.append(w['hepburn'])\n",
    "                            name_1 = ' '.join(jps)\n",
    "                            name_1 = re.sub(r'\\s{2,}', ' ', name_1).strip()\n",
    "                            name = split_name(name_1)\n",
    "                else:\n",
    "                    name = split_name(name)\n",
    "            else:\n",
    "                name = ['NULL']\n",
    "            names_list.extend(name)\n",
    "            \n",
    "            org = author.get('org')\n",
    "            if org != '':\n",
    "                org = clean_text(org)\n",
    "                org = org.split(' ')\n",
    "                org = [re.sub(r'\\d+', '0', word) for word in org]\n",
    "                if len(org) == 0:\n",
    "                    org = ['NULL']\n",
    "            else:\n",
    "                org = ['NULL']\n",
    "            orgs_list.extend(org)\n",
    "            name = ','.join(list(set(name)))\n",
    "            org = ' '.join(list(set(org)))\n",
    "            cleaned_authors.append([name, org])\n",
    "    return cleaned_authors, list(set(names_list)), list(set(orgs_list))\n",
    "\n",
    "def clean_year(year):\n",
    "    if year == 0 or year == '':\n",
    "        return None\n",
    "    else:\n",
    "        return int(year)\n",
    "\n",
    "def split_name(name):\n",
    "    name = name.split(' ')\n",
    "    name = [n for n in name if n.isdigit() != True]\n",
    "    if len(name) == 2:\n",
    "        name = [\n",
    "            name[0] + ' ' + name[1],\n",
    "            name[1] + ' ' + name[0],\n",
    "        ]\n",
    "        name.sort()\n",
    "        name = name[0]\n",
    "    elif len(name) > 2:\n",
    "        name = [\n",
    "            name[0] + ' ' + name[1] + ' ' + name[2],\n",
    "            name[2] + ' ' +  name[1] + ' ' + name[0],\n",
    "        ]\n",
    "        name.sort()\n",
    "        name = name[0]\n",
    "    elif len(name) == 1 and name != ['']:\n",
    "        name = name[0]\n",
    "    else:\n",
    "        name = 'NULL'\n",
    "    return [name]\n",
    "\n",
    "def clean_jp(lst):\n",
    "    lst = [neologdn.normalize(word) for word in lst]\n",
    "    lst = list(itertools.chain.from_iterable([list(tk.tokenize(txt, wakati=True)) for word in lst]))\n",
    "    lst = [word for word in lst if word != '']\n",
    "    return lst\n",
    "    \n",
    "def clean_zh(lst):\n",
    "    lst = list(itertools.chain.from_iterable([jieba.lcut(word) for word in lst]))\n",
    "    lst = [word for word in lst if word != '']\n",
    "    return lst\n",
    "\n",
    "def judge_lang(txt, lst):\n",
    "    ptn = r'[\\p{Block=Hiragana}\\p{Script=Katakana}\\p{Script_Extensions=Han}\\u2E80-\\u2FDF\\u3005-\\u3007\\u3400-\\u4DBF\\u4E00-\\u9FFF\\uF900-\\uFAFF\\U00020000-\\U0002EBEF]+'\n",
    "    re_ptn = regex.compile(ptn)\n",
    "    if len(lst) == 0:\n",
    "        lang = 'Nothing'\n",
    "    else:\n",
    "        re_txt = re_ptn.search(txt)\n",
    "        if re_txt != None:\n",
    "            lang = ftlangdetect.detect(re_txt.group(), low_memory=False)['lang']\n",
    "            if lang == 'jp':\n",
    "                lst = clean_jp(lst)\n",
    "            elif lang == 'zh':\n",
    "                lst = clean_zh(lst)\n",
    "        else:\n",
    "            lang = ftlangdetect.detect(txt, low_memory=False)['lang']\n",
    "            if lang not in ['en','de','zh','fr','es','ru','it','Nothing']:\n",
    "                lang = 'Other'\n",
    "    return lst, lang\n",
    "    \n",
    "def is_zh(in_str):\n",
    "    \"\"\"\n",
    "    >>> is_zh(u'おはよう')\n",
    "    False\n",
    "    >>> is_zh(u'&#35828;地')\n",
    "    True\n",
    "    \"\"\"\n",
    "    questions_before = [s for s in in_str]\n",
    "    questions_gb2312 = [s for s in \\\n",
    "        in_str.encode('gb2312','ignore').decode('gb2312')]\n",
    "    questions_cp932 = [s for s in \\\n",
    "        in_str.encode('cp932','ignore').decode('cp932')]\n",
    "    if (questions_gb2312 == questions_before) and (\n",
    "        (set(questions_before) - set(questions_cp932)) != set([])):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf557d06-c254-4757-8313-d7a9c9b1bb9a",
   "metadata": {},
   "source": [
    "# pid_to_info_all.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38ddc6e3-23ed-4039-a8e2-def8f512a206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6IsfnuWU</td>\n",
       "      <td>Probabilistic Skyline Operator over Sliding Wi...</td>\n",
       "      <td>[{'name': 'Wenjie Zhang', 'org': 'UNSW Sydney'...</td>\n",
       "      <td>Skyline computation has many applications incl...</td>\n",
       "      <td>[continuous skyline query, probabilistic skyli...</td>\n",
       "      <td>ICDE '09 Proceedings of the 2009 IEEE Internat...</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8B8GhlnI</td>\n",
       "      <td>Editorial: Knowledge-Driven Activity Recogniti...</td>\n",
       "      <td>[{'name': 'Liming Chen', 'org': ''}, {'name': ...</td>\n",
       "      <td></td>\n",
       "      <td>[activity recognition]</td>\n",
       "      <td>Periodicals</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4dZKGwVR</td>\n",
       "      <td>Subscriber Assignment For Wide-Area Content-Ba...</td>\n",
       "      <td>[{'name': 'Albert Yu', 'org': 'Duke Univ, Dept...</td>\n",
       "      <td>We study the problem of assigning subscribers ...</td>\n",
       "      <td>[Monte Carlo approximation algorithm, future e...</td>\n",
       "      <td>ICDE '11 Proceedings of the 2011 IEEE 27th Int...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V1JgT3OM</td>\n",
       "      <td>Tree-Based Mining for Discovering Patterns of ...</td>\n",
       "      <td>[{'name': 'Zhiwen Yu', 'org': 'Northwestern Po...</td>\n",
       "      <td>AbstractDiscovering semantic knowledge is sign...</td>\n",
       "      <td>[discovering patterns, interaction flow patter...</td>\n",
       "      <td>Periodicals</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HMvrPr2W</td>\n",
       "      <td>Protein Function Prediction using Multi-label ...</td>\n",
       "      <td>[{'name': 'Guoxian Yu', 'org': 'Southwest Univ...</td>\n",
       "      <td>AbstractHigh-throughput experimental technique...</td>\n",
       "      <td>[heterogeneous proteomic data sets, multilabel...</td>\n",
       "      <td>IEEE/ACM Transactions on Computational Biology...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  6IsfnuWU  Probabilistic Skyline Operator over Sliding Wi...   \n",
       "1  8B8GhlnI  Editorial: Knowledge-Driven Activity Recogniti...   \n",
       "2  4dZKGwVR  Subscriber Assignment For Wide-Area Content-Ba...   \n",
       "3  V1JgT3OM  Tree-Based Mining for Discovering Patterns of ...   \n",
       "4  HMvrPr2W  Protein Function Prediction using Multi-label ...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'name': 'Wenjie Zhang', 'org': 'UNSW Sydney'...   \n",
       "1  [{'name': 'Liming Chen', 'org': ''}, {'name': ...   \n",
       "2  [{'name': 'Albert Yu', 'org': 'Duke Univ, Dept...   \n",
       "3  [{'name': 'Zhiwen Yu', 'org': 'Northwestern Po...   \n",
       "4  [{'name': 'Guoxian Yu', 'org': 'Southwest Univ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Skyline computation has many applications incl...   \n",
       "1                                                      \n",
       "2  We study the problem of assigning subscribers ...   \n",
       "3  AbstractDiscovering semantic knowledge is sign...   \n",
       "4  AbstractHigh-throughput experimental technique...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [continuous skyline query, probabilistic skyli...   \n",
       "1                             [activity recognition]   \n",
       "2  [Monte Carlo approximation algorithm, future e...   \n",
       "3  [discovering patterns, interaction flow patter...   \n",
       "4  [heterogeneous proteomic data sets, multilabel...   \n",
       "\n",
       "                                               venue  year  \n",
       "0  ICDE '09 Proceedings of the 2009 IEEE Internat...  2009  \n",
       "1                                        Periodicals  2011  \n",
       "2  ICDE '11 Proceedings of the 2011 IEEE 27th Int...  2011  \n",
       "3                                        Periodicals  2012  \n",
       "4  IEEE/ACM Transactions on Computational Biology...  2013  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../raw/pid_to_info_all.json\"\n",
    "df = pd.read_json(path)\n",
    "df = df.T\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb7de040-110c-45e6-83a5-62d3c6fefc1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "317302it [03:43, 1417.28it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = []\n",
    "for index, row in tqdm.tqdm(df.iterrows()):\n",
    "    p_id = row['id']\n",
    "    # 前処理&リスト化&日本語・中国語対応：title\n",
    "    row['title'] = clean_text(row['title'])\n",
    "    title = ' '.join(split_text(row['title']))\n",
    "\n",
    "    #前処理&リスト化・著者数:authors\n",
    "    authors,names,orgs = clean_authors(row['authors'])\n",
    "    \n",
    "    # 前処理&リスト化&日本語・中国語対応：abstract\n",
    "    row['abstract'] = clean_text(row['abstract'])\n",
    "    abstract = split_text(row['abstract'])\n",
    "    abstract = ' '.join(split_text(row['abstract']))\n",
    "    \n",
    "    # 前処理&リスト化&日本語・中国語対応：keywords\n",
    "    row['keywords'] = [clean_text(keyword) for keyword in row['keywords']]\n",
    "    keywords = ','.join(row['keywords'])\n",
    "\n",
    "    # 前処理&リスト化&日本語・中国語対応：venue\n",
    "    row['venue'] = clean_text(row['venue'])\n",
    "    venue = ' '.join(split_text(row['venue']))\n",
    "    cleaned_data.append([\n",
    "        row['id'],\n",
    "        title,\n",
    "        authors,\n",
    "        abstract,\n",
    "        keywords,\n",
    "        venue,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4c22de5-a093-48a5-9b50-98e7a57d0168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317302, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>title</th><th>authors</th><th>abstract</th><th>keywords</th><th>venue</th></tr><tr><td>str</td><td>str</td><td>list[list[str]]</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;6IsfnuWU&quot;</td><td>&quot;probabilistic skyline operator…</td><td>[[&quot;wenjie zhang&quot;, &quot;unsw sydney&quot;], [&quot;lin xuemin&quot;, &quot;unsw sydney&quot;], … [&quot;jeffrey xu yu&quot;, &quot;of chinese hong kong university&quot;]]</td><td>&quot;skyline computation has many a…</td><td>&quot;continuous skyline query,proba…</td><td>&quot;icde 0 proceedings of the 0 ie…</td></tr><tr><td>&quot;8B8GhlnI&quot;</td><td>&quot;editorial knowledge driven act…</td><td>[[&quot;chen liming&quot;, &quot;NULL&quot;], [&quot;chris nugent&quot;, &quot;computing of school&quot;], … [&quot;yu zhiwen&quot;, &quot;NULL&quot;]]</td><td>&quot;&quot;</td><td>&quot;activity recognition&quot;</td><td>&quot;periodicals&quot;</td></tr><tr><td>&quot;4dZKGwVR&quot;</td><td>&quot;subscriber assignment for wide…</td><td>[[&quot;albert yu&quot;, &quot;nc duke comp durham usa sci dept 0 univ&quot;], [&quot;agarwal k pankaj&quot;, &quot;nc duke comp durham usa sci dept 0 univ&quot;], [&quot;jun yang&quot;, &quot;nc duke comp durham usa sci dept 0 univ&quot;]]</td><td>&quot;we study the problem of assign…</td><td>&quot;monte carlo approximation algo…</td><td>&quot;icde 0 proceedings of the 0 ie…</td></tr><tr><td>&quot;V1JgT3OM&quot;</td><td>&quot;tree based mining for discover…</td><td>[[&quot;yu zhiwen&quot;, &quot;northwestern an polytechnical university xi&quot;], [&quot;yu zhiyong&quot;, &quot;fuzhou university&quot;], … [&quot;nakamura yuichi&quot;, &quot;university kyoto&quot;]]</td><td>&quot;abstractdiscovering semantic k…</td><td>&quot;discovering patterns,interacti…</td><td>&quot;periodicals&quot;</td></tr><tr><td>&quot;HMvrPr2W&quot;</td><td>&quot;protein function prediction us…</td><td>[[&quot;guoxian yu&quot;, &quot;guangzhou of south technology and southwest china beibei university&quot;], [&quot;huzefa rangwala&quot;, &quot;george fairfax university mason&quot;], … [&quot;yu zhiwen&quot;, &quot;guangzhou of south technology china university&quot;]]</td><td>&quot;abstracthigh throughput experi…</td><td>&quot;heterogeneous proteomic data s…</td><td>&quot;ieee acm transactions on compu…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌──────────┬─────────────────┬─────────────────┬─────────────────┬────────────────┬────────────────┐\n",
       "│ id       ┆ title           ┆ authors         ┆ abstract        ┆ keywords       ┆ venue          │\n",
       "│ ---      ┆ ---             ┆ ---             ┆ ---             ┆ ---            ┆ ---            │\n",
       "│ str      ┆ str             ┆ list[list[str]] ┆ str             ┆ str            ┆ str            │\n",
       "╞══════════╪═════════════════╪═════════════════╪═════════════════╪════════════════╪════════════════╡\n",
       "│ 6IsfnuWU ┆ probabilistic   ┆ [[\"wenjie       ┆ skyline         ┆ continuous     ┆ icde 0         │\n",
       "│          ┆ skyline         ┆ zhang\", \"unsw   ┆ computation has ┆ skyline        ┆ proceedings of │\n",
       "│          ┆ operator…       ┆ sydney…         ┆ many a…         ┆ query,proba…   ┆ the 0 ie…      │\n",
       "│ 8B8GhlnI ┆ editorial       ┆ [[\"chen         ┆                 ┆ activity       ┆ periodicals    │\n",
       "│          ┆ knowledge       ┆ liming\",        ┆                 ┆ recognition    ┆                │\n",
       "│          ┆ driven act…     ┆ \"NULL\"], [\"ch…  ┆                 ┆                ┆                │\n",
       "│ 4dZKGwVR ┆ subscriber      ┆ [[\"albert yu\",  ┆ we study the    ┆ monte carlo    ┆ icde 0         │\n",
       "│          ┆ assignment for  ┆ \"nc duke comp   ┆ problem of      ┆ approximation  ┆ proceedings of │\n",
       "│          ┆ wide…           ┆ d…              ┆ assign…         ┆ algo…          ┆ the 0 ie…      │\n",
       "│ V1JgT3OM ┆ tree based      ┆ [[\"yu zhiwen\",  ┆ abstractdiscove ┆ discovering    ┆ periodicals    │\n",
       "│          ┆ mining for      ┆ \"northwestern   ┆ ring semantic   ┆ patterns,inter ┆                │\n",
       "│          ┆ discover…       ┆ a…              ┆ k…              ┆ acti…          ┆                │\n",
       "│ HMvrPr2W ┆ protein         ┆ [[\"guoxian yu\", ┆ abstracthigh    ┆ heterogeneous  ┆ ieee acm       │\n",
       "│          ┆ function        ┆ \"guangzhou of … ┆ throughput      ┆ proteomic data ┆ transactions   │\n",
       "│          ┆ prediction us…  ┆                 ┆ experi…         ┆ s…             ┆ on compu…      │\n",
       "└──────────┴─────────────────┴─────────────────┴─────────────────┴────────────────┴────────────────┘"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.DataFrame(cleaned_data, schema=['id','title','authors','abstract','keywords','venue'])\n",
    "df.write_parquet('../test_data/cleaned_pid_to_info_all_v6_light.parquet')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
